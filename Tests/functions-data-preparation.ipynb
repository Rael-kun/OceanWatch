{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8791167,"sourceType":"datasetVersion","datasetId":5285501}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-12T20:02:41.881393Z","iopub.execute_input":"2024-07-12T20:02:41.881897Z","iopub.status.idle":"2024-07-12T20:02:42.612456Z","shell.execute_reply.started":"2024-07-12T20:02:41.881859Z","shell.execute_reply":"2024-07-12T20:02:42.609444Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/ais-baltic-sea/dirways_all_2018_2019.csv\n/kaggle/input/ais-baltic-sea/validation_set_winter.csv\n/kaggle/input/ais-baltic-sea/training_set.csv\n/kaggle/input/ais-baltic-sea/validation_set_summer.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\n#import matplotlib.pyplot as plt\n# %matplotlib inline \n# from mpl_toolkits.basemap import Basemap  # import Basemap matplotlib toolkit\n\nDATA_DIR = \"/kaggle/input/ais-baltic-sea/\"\n\n# dtype_spec = {\n#     'iceclass': str,\n# }\nais = pd.read_csv(DATA_DIR + 'training_set.csv', index_col=0) #parse_dates=['timestamp'], dtype=dtype_spec","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:02:42.617870Z","iopub.execute_input":"2024-07-12T20:02:42.619779Z","iopub.status.idle":"2024-07-12T20:03:58.557210Z","shell.execute_reply.started":"2024-07-12T20:02:42.619693Z","shell.execute_reply":"2024-07-12T20:03:58.555810Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_64/791605484.py:14: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n  ais = pd.read_csv(DATA_DIR + 'training_set.csv', index_col=0) #parse_dates=['timestamp'], dtype=dtype_spec\n","output_type":"stream"}]},{"cell_type":"code","source":"#Print headings and description about the dataset\nais.head()\n#ais.describe()\n#print(ais['course'].describe())","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:03:58.558688Z","iopub.execute_input":"2024-07-12T20:03:58.559063Z","iopub.status.idle":"2024-07-12T20:03:58.599684Z","shell.execute_reply.started":"2024-07-12T20:03:58.559032Z","shell.execute_reply":"2024-07-12T20:03:58.597798Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"             timestamp       mmsi        lat        lon  speed  course  \\\n0  2017-11-02 11:19:07  205366000  54.347242   9.991140   4.01    56.6   \n1  2017-11-02 11:30:58  205366000  54.360348  10.025337   4.17    77.5   \n2  2017-11-02 11:37:58  205366000  54.359573  10.049203   4.48    92.4   \n3  2017-11-02 11:44:07  205366000  54.365497  10.071320   3.81    44.9   \n4  2017-11-02 11:56:08  205366000  54.370437  10.109498   2.73   103.6   \n\n   heading  turnrate  breadth vessel_type  vessel_max_speed  draft   power  \\\n0     56.0       0.0    21.33           T              16.0   8.18  5820.0   \n1     81.0       0.0    21.33           T              16.0   8.18  5820.0   \n2     92.0       0.0    21.33           T              16.0   8.18  5820.0   \n3     46.0       0.0    21.33           T              16.0   8.18  5820.0   \n4    103.0       0.0    21.33           T              16.0   8.18  5820.0   \n\n       dwt iceclass  \n0  13289.0       IA  \n1  13289.0       IA  \n2  13289.0       IA  \n3  13289.0       IA  \n4  13289.0       IA  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>mmsi</th>\n      <th>lat</th>\n      <th>lon</th>\n      <th>speed</th>\n      <th>course</th>\n      <th>heading</th>\n      <th>turnrate</th>\n      <th>breadth</th>\n      <th>vessel_type</th>\n      <th>vessel_max_speed</th>\n      <th>draft</th>\n      <th>power</th>\n      <th>dwt</th>\n      <th>iceclass</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-11-02 11:19:07</td>\n      <td>205366000</td>\n      <td>54.347242</td>\n      <td>9.991140</td>\n      <td>4.01</td>\n      <td>56.6</td>\n      <td>56.0</td>\n      <td>0.0</td>\n      <td>21.33</td>\n      <td>T</td>\n      <td>16.0</td>\n      <td>8.18</td>\n      <td>5820.0</td>\n      <td>13289.0</td>\n      <td>IA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-11-02 11:30:58</td>\n      <td>205366000</td>\n      <td>54.360348</td>\n      <td>10.025337</td>\n      <td>4.17</td>\n      <td>77.5</td>\n      <td>81.0</td>\n      <td>0.0</td>\n      <td>21.33</td>\n      <td>T</td>\n      <td>16.0</td>\n      <td>8.18</td>\n      <td>5820.0</td>\n      <td>13289.0</td>\n      <td>IA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-11-02 11:37:58</td>\n      <td>205366000</td>\n      <td>54.359573</td>\n      <td>10.049203</td>\n      <td>4.48</td>\n      <td>92.4</td>\n      <td>92.0</td>\n      <td>0.0</td>\n      <td>21.33</td>\n      <td>T</td>\n      <td>16.0</td>\n      <td>8.18</td>\n      <td>5820.0</td>\n      <td>13289.0</td>\n      <td>IA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-11-02 11:44:07</td>\n      <td>205366000</td>\n      <td>54.365497</td>\n      <td>10.071320</td>\n      <td>3.81</td>\n      <td>44.9</td>\n      <td>46.0</td>\n      <td>0.0</td>\n      <td>21.33</td>\n      <td>T</td>\n      <td>16.0</td>\n      <td>8.18</td>\n      <td>5820.0</td>\n      <td>13289.0</td>\n      <td>IA</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-11-02 11:56:08</td>\n      <td>205366000</td>\n      <td>54.370437</td>\n      <td>10.109498</td>\n      <td>2.73</td>\n      <td>103.6</td>\n      <td>103.0</td>\n      <td>0.0</td>\n      <td>21.33</td>\n      <td>T</td>\n      <td>16.0</td>\n      <td>8.18</td>\n      <td>5820.0</td>\n      <td>13289.0</td>\n      <td>IA</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# PRE-PROCESSING****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport datetime\n\ndef preprocess(data):\n    # Remove unnecessary columns\n    columns_to_keep = ['lat', 'lon', 'speed', 'course', 'timestamp', 'mmsi']\n    data = data[columns_to_keep]\n    \n    # Drop rows with any null values in their columns\n    data = data.dropna()\n\n    # Convert 'timestamp' column to datetime objects\n    data['timestamp'] = pd.to_datetime(data['timestamp'])\n\n    # Convert datetime objects to UNIX timestamp (seconds since the Unix epoch)\n    data['timestamp_unix'] = data['timestamp'].apply(lambda x: int(x.timestamp()))\n\n    # Sort by 'MMSI' and 'timestamp' columns\n    data = data.sort_values(by=['mmsi', 'timestamp'])\n\n    # Calculate timestep (time difference) between consecutive messages for each MMSI\n    data['timestep_seconds'] = data.groupby('mmsi')['timestamp'].diff().dt.total_seconds()\n    \n    # Define conditions for incrementing the path counter\n    # first condition is when the time interval path of a ship goes over an hour\n    # second condition is when there is a change in MMSI\n    conditions = (data['timestep_seconds'] > 3600) | (data['mmsi'].ne(data['mmsi'].shift()))\n\n    # Create path counter based on conditions\n    data['path'] = conditions.cumsum()\n    # Now ais DataFrame will have a new column 'path' which increments\n    # whenever the conditions (time difference > 1 hour or MMSI changes) are met.\n    \n    # Group by 'path' and count the number of messages in each path\n    path_counts = data.groupby('path').size()\n\n    # Create a new DataFrame with 'path' as index and 'pathCount' as the count of messages\n    path_count_table = pd.DataFrame({'pathCount': path_counts})\n    path_count_table.head()\n    # The index of path_count_table will automatically be the path numbers (since 'path' was used as index)\n    # path_count_table now holds the path number as index and the count of messages (pathCount) as the result\n\n    # Filter paths with lengths less than 24 or greater than 144\n    filtered_paths = path_counts[(path_counts < 24) | (path_counts > 144)].index\n\n    # Convert the index (path numbers) to an array\n    filtered_paths_array = filtered_paths.to_numpy()\n    # filtered_paths_array now holds the path numbers with lengths outside the specified range\n    #print(filtered_paths_array)\n\n    # Eliminate all invalid paths\n    #'filtered_paths_array' contains the invalid path numbers as calculated previously\n    # Step 1: Sort the filtered paths array\n    filtered_paths_array = sorted(filtered_paths_array)\n    # Step 2: Filter the ais_sorted DataFrame to remove rows with invalid paths\n    data = data[~data['path'].isin(filtered_paths_array)]\n    # Now ais_valid contains only valid paths\n    #print(len(ais_valid))\n    #print(len(ais))\n\n    return data\n\n\n# Preprocess the dataset\nais_valid = preprocess(ais)\nais_valid.head()\n#print(ais_valid[ais_valid['mmsi'] == 205366000])\n#print(ais_valid['path'].max())","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:03:58.601769Z","iopub.execute_input":"2024-07-12T20:03:58.602311Z","iopub.status.idle":"2024-07-12T20:05:42.281501Z","shell.execute_reply.started":"2024-07-12T20:03:58.602242Z","shell.execute_reply":"2024-07-12T20:05:42.279044Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           lat        lon  speed  course           timestamp       mmsi  \\\n922  57.756818  10.420237   9.16    78.0 2017-12-22 10:23:57  205366000   \n923  57.767407  10.509043   8.90    77.1 2017-12-22 10:34:03  205366000   \n924  57.777053  10.594123   8.18    83.0 2017-12-22 10:45:13  205366000   \n925  57.776770  10.678505   8.33    92.8 2017-12-22 10:55:19  205366000   \n926  57.768455  10.734610   3.55   156.2 2017-12-22 11:04:30  205366000   \n\n     timestamp_unix  timestep_seconds  path  \n922      1513938237         1973822.0     4  \n923      1513938843             606.0     4  \n924      1513939513             670.0     4  \n925      1513940119             606.0     4  \n926      1513940670             551.0     4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lat</th>\n      <th>lon</th>\n      <th>speed</th>\n      <th>course</th>\n      <th>timestamp</th>\n      <th>mmsi</th>\n      <th>timestamp_unix</th>\n      <th>timestep_seconds</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>922</th>\n      <td>57.756818</td>\n      <td>10.420237</td>\n      <td>9.16</td>\n      <td>78.0</td>\n      <td>2017-12-22 10:23:57</td>\n      <td>205366000</td>\n      <td>1513938237</td>\n      <td>1973822.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>923</th>\n      <td>57.767407</td>\n      <td>10.509043</td>\n      <td>8.90</td>\n      <td>77.1</td>\n      <td>2017-12-22 10:34:03</td>\n      <td>205366000</td>\n      <td>1513938843</td>\n      <td>606.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>924</th>\n      <td>57.777053</td>\n      <td>10.594123</td>\n      <td>8.18</td>\n      <td>83.0</td>\n      <td>2017-12-22 10:45:13</td>\n      <td>205366000</td>\n      <td>1513939513</td>\n      <td>670.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>925</th>\n      <td>57.776770</td>\n      <td>10.678505</td>\n      <td>8.33</td>\n      <td>92.8</td>\n      <td>2017-12-22 10:55:19</td>\n      <td>205366000</td>\n      <td>1513940119</td>\n      <td>606.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>926</th>\n      <td>57.768455</td>\n      <td>10.734610</td>\n      <td>3.55</td>\n      <td>156.2</td>\n      <td>2017-12-22 11:04:30</td>\n      <td>205366000</td>\n      <td>1513940670</td>\n      <td>551.0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# TRAIN-TEST-SPLIT****","metadata":{}},{"cell_type":"code","source":"#Now we use ais_valid to provide train,test and validation sets\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Step 1: Split the data into 80% train and 20% remaining\ntrain_data, remaining_data = train_test_split(ais_valid, test_size=0.20, random_state=42)\n\n# Step 2: Split the remaining 20% into 10% test and 10% validation\ntest_data, val_data = train_test_split(remaining_data, test_size=0.50, random_state=42)\n\n# Now we have 80% train, 10% test, and 10% validation sets\nprint(f\"Train data size: {train_data.shape}\")\nprint(f\"Test data size: {test_data.shape}\")\nprint(f\"Validation data size: {val_data.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:05:42.288335Z","iopub.execute_input":"2024-07-12T20:05:42.289099Z","iopub.status.idle":"2024-07-12T20:05:44.101034Z","shell.execute_reply.started":"2024-07-12T20:05:42.289036Z","shell.execute_reply":"2024-07-12T20:05:44.099003Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Train data size: (891604, 9)\nTest data size: (111450, 9)\nValidation data size: (111451, 9)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# NORMALIZATION****","metadata":{}},{"cell_type":"code","source":"# Normalize values, specifically:\n#   Lat = (lat - lat_min) / (lat_max-lat_min)\n#   Lon = (lon - lon_min) / (lon_max-lon_min)\n#   Speed = speed / 52.8\n#   Course = course / 360\n\n# Calculate min and max for latitude and longitude\nlat_min = ais_valid['lat'].min()\nlat_max = ais_valid['lat'].max()\nlon_min = ais_valid['lon'].min()\nlon_max = ais_valid['lon'].max()\n\n# print(f\"Latitude: min = {lat_min}, max = {lat_max}\")\n# print(f\"Longitude: min = {lon_min}, max = {lon_max}\")\n\n# Define the normalization function\ndef normalize_data(df, lat_min, lat_max, lon_min, lon_max):\n    df['alat'] = (df['lat'] - lat_min) / (lat_max - lat_min)\n    df['alon'] = (df['lon'] - lon_min) / (lon_max - lon_min)\n    df['aspeed'] = df['speed'] / 52.8\n    df['acourse'] = df['course'] / 360\n    return df\n\n# Apply the normalization to train, test, and validation datasets\ntrain_data = normalize_data(train_data, lat_min, lat_max, lon_min, lon_max)\ntest_data = normalize_data(test_data, lat_min, lat_max, lon_min, lon_max)\nval_data = normalize_data(val_data, lat_min, lat_max, lon_min, lon_max)\n\n#print(train_data.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:05:44.104162Z","iopub.execute_input":"2024-07-12T20:05:44.104764Z","iopub.status.idle":"2024-07-12T20:05:44.188336Z","shell.execute_reply.started":"2024-07-12T20:05:44.104718Z","shell.execute_reply":"2024-07-12T20:05:44.186766Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# PATH INI: PROPER INDEXING FOR DICTIONARIES****","metadata":{}},{"cell_type":"code","source":"#For each set, create a new pathIndex called pathIni and start this index at 0,incrementing every time the path changes \n#this helps indexing in the dictionary later\n\n# Function to create the pathIni index\ndef create_path_ini(df):\n    df = df.sort_values(by='path').reset_index(drop=True)  # Ensure the data is sorted by 'path'\n    unique_paths = df['path'].unique()  # Get unique paths\n    path_mapping = {path: idx for idx, path in enumerate(unique_paths)}  # Create a mapping from old path to new index\n    df['pathIni'] = df['path'].map(path_mapping)  # Map old paths to new pathIni indices\n    return df\n\n# Apply the function to each dataset\ntrain_data = create_path_ini(train_data)\ntest_data = create_path_ini(test_data)\nval_data = create_path_ini(val_data)\n\n#print(train_data.head())\n#print(test_data.head())\n#print(val_data.head())\n\n#train_data[\"pathIni\"].max()\n#test_data[\"pathIni\"].max()\n#val_data[\"pathIni\"].max()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:05:44.190365Z","iopub.execute_input":"2024-07-12T20:05:44.190951Z","iopub.status.idle":"2024-07-12T20:05:45.062154Z","shell.execute_reply.started":"2024-07-12T20:05:44.190901Z","shell.execute_reply":"2024-07-12T20:05:45.060323Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# TRANSFORMING FRAMES INTO CSV FILES****","metadata":{}},{"cell_type":"code","source":"train_data.to_csv(\"marinedata_train.csv\", index=False)\ntest_data.to_csv(\"marinedata_test.csv\", index=False)\nval_data.to_csv(\"marinedata_val.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:05:45.063824Z","iopub.execute_input":"2024-07-12T20:05:45.064226Z","iopub.status.idle":"2024-07-12T20:06:11.601927Z","shell.execute_reply.started":"2024-07-12T20:05:45.064196Z","shell.execute_reply":"2024-07-12T20:06:11.599072Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# LOADING CSV FILES****","metadata":{}},{"cell_type":"code","source":"########################### Use proper datapath here\ndataset_path = \"/kaggle/working/\"\nl_csv_filename =[\"marinedata_train.csv\",\n                 \"marinedata_test.csv\",\n                \"marinedata_val.csv\"]\n\n#pkl_filename = \"marinedata_train.pkl\"\npkl_filename_train = \"marinedata_train.pkl\"\npkl_filename_test  = \"marinedata_test.pkl\"\npkl_filename_valid = \"marinedata_val.pkl\"\n\n#Column indices for easier reference\nALAT, ALON, ASPEED, ACOURSE, TIMESTAMP, MMSI, PATH = list(range(7))\n\n## LOADING CSV FILES\n#======================================\nm_msg_train = []\nm_msg_test = []\nm_msg_valid = []\n\nfor csv_filename in l_csv_filename:\n    data_path = os.path.join(dataset_path,csv_filename)\n    f = pd.read_csv(data_path)\n    print(\"Reading \", csv_filename, \"...\")\n    \n    l_l_msg = [] # list of AIS messages, each row is a message (list of AIS attributes)\n    for i in range(len(f[\"mmsi\"])):\n        row = f.iloc[i]\n        l_l_msg.append([float(row[\"alat\"]), float(row[\"alon\"]), float(row[\"aspeed\"]),\n                            float(row[\"acourse\"]),\n                            int(row[\"timestamp_unix\"]), int(row[\"mmsi\"]),\n                           int(row[\"pathIni\"])])\n    if csv_filename == \"marinedata_train.csv\":\n        m_msg_train = np.array(l_l_msg)\n        print(\"Total number of AIS messages: \",m_msg_train.shape[0])\n    if csv_filename == \"marinedata_test.csv\":\n        m_msg_test = np.array(l_l_msg)\n        print(\"Total number of AIS messages: \",m_msg_test.shape[0])\n    if csv_filename == \"marinedata_val.csv\":\n        m_msg_valid = np.array(l_l_msg)\n        print(\"Total number of AIS messages: \",m_msg_valid.shape[0])\n\n\n## FILTERING \n#======================================\n# Selecting AIS messages in the ROI and in the period of interest.\n\nprint(\"Number of msgs in the training set: \",len(m_msg_train))\nprint(\"Number of msgs in the test set: \",len(m_msg_test))\nprint(\"Number of msgs in the validation set: \",len(m_msg_valid))\n\n## MERGING INTO DICT\nprint(\"Convert to dicts of vessel's tracks...\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:06:11.604875Z","iopub.execute_input":"2024-07-12T20:06:11.607856Z","iopub.status.idle":"2024-07-12T20:08:31.259892Z","shell.execute_reply.started":"2024-07-12T20:06:11.607659Z","shell.execute_reply":"2024-07-12T20:08:31.258016Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Reading  marinedata_train.csv ...\nTotal number of AIS messages:  891604\nReading  marinedata_test.csv ...\nTotal number of AIS messages:  111450\nReading  marinedata_val.csv ...\nTotal number of AIS messages:  111451\nNumber of msgs in the training set:  891604\nNumber of msgs in the test set:  111450\nNumber of msgs in the validation set:  111451\nConvert to dicts of vessel's tracks...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# CONVERT MESSAGES TO DICTIONARIES****","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm                #tqdm can be used to display progress bars\n\n# Training set\nVs_train = dict()\nfor v_msg in tqdm(m_msg_train):\n    mmsi = int(v_msg[MMSI])\n    pathIni = int(v_msg[PATH])\n    if not (pathIni in list(Vs_train.keys())):\n        Vs_train[pathIni] = {'mmsi': mmsi, 'traj': np.empty((0,6))}\n    Vs_train[pathIni] = {'mmsi': mmsi, 'traj': np.concatenate((Vs_train[pathIni]['traj'], np.expand_dims(v_msg[:6],0)), axis = 0)}\nfor key in tqdm(list(Vs_train.keys())):\n    mmsi = int(Vs_train[key]['traj'][0][5])\n    Vs_train[key] = {'mmsi': mmsi,'traj': np.array(sorted(Vs_train[key]['traj'], key=lambda m_entry: m_entry[TIMESTAMP]))}\n\n# Validation set\nVs_valid = dict()\nfor v_msg in tqdm(m_msg_valid):\n    mmsi = int(v_msg[MMSI])\n    pathIni = int(v_msg[PATH])\n    if not (pathIni in list(Vs_valid.keys())):\n        Vs_valid[pathIni] = {'mmsi': mmsi, 'traj': np.empty((0,6))}\n    Vs_valid[pathIni] = {'mmsi': mmsi, 'traj': np.concatenate((Vs_valid[pathIni]['traj'], np.expand_dims(v_msg[:6],0)), axis = 0)}\nfor key in tqdm(list(Vs_valid.keys())):\n    mmsi = int(Vs_valid[key]['traj'][0][5])\n    Vs_valid[key] = {'mmsi': mmsi,'traj': np.array(sorted(Vs_valid[key]['traj'], key=lambda m_entry: m_entry[TIMESTAMP]))}\n\n# Test set\nVs_test = dict()\nfor v_msg in tqdm(m_msg_test):\n    mmsi = int(v_msg[MMSI])\n    pathIni = int(v_msg[PATH])\n    if not (pathIni in list(Vs_test.keys())):\n        Vs_test[pathIni] = {'mmsi': mmsi, 'traj': np.empty((0,6))}\n    Vs_test[pathIni] = {'mmsi': mmsi, 'traj': np.concatenate((Vs_test[pathIni]['traj'], np.expand_dims(v_msg[:6],0)), axis = 0)}\nfor key in tqdm(list(Vs_test.keys())):\n    mmsi = int(Vs_test[key]['traj'][0][5])\n    Vs_test[key] = {'mmsi': mmsi,'traj': np.array(sorted(Vs_test[key]['traj'], key=lambda m_entry: m_entry[TIMESTAMP]))}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:08:31.262152Z","iopub.execute_input":"2024-07-12T20:08:31.263068Z","iopub.status.idle":"2024-07-12T20:13:16.642403Z","shell.execute_reply.started":"2024-07-12T20:08:31.263012Z","shell.execute_reply":"2024-07-12T20:13:16.640643Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 891604/891604 [03:47<00:00, 3914.25it/s] \n100%|██████████| 14855/14855 [00:01<00:00, 10711.96it/s]\n100%|██████████| 111451/111451 [00:28<00:00, 3975.17it/s]\n100%|██████████| 14620/14620 [00:00<00:00, 64725.14it/s]\n100%|██████████| 111450/111450 [00:27<00:00, 4030.36it/s]\n100%|██████████| 14612/14612 [00:00<00:00, 64486.07it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# PICKLING****","metadata":{}},{"cell_type":"code","source":"# SAVING TO PICKLE FILES\n#======================================\nimport pickle\n\nwith open(pkl_filename_train, 'wb') as f:\n    pickle.dump(m_msg_train, f)\n    print(f\"Training data saved to {pkl_filename_train}\")\n\nwith open(pkl_filename_test, 'wb') as f:\n    pickle.dump(m_msg_test, f)\n    print(f\"Test data saved to {pkl_filename_test}\")\n\nwith open(pkl_filename_valid, 'wb') as f:\n    pickle.dump(m_msg_valid, f)\n    print(f\"Validation data saved to {pkl_filename_valid}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:13:16.644344Z","iopub.execute_input":"2024-07-12T20:13:16.644837Z","iopub.status.idle":"2024-07-12T20:13:16.751556Z","shell.execute_reply.started":"2024-07-12T20:13:16.644800Z","shell.execute_reply":"2024-07-12T20:13:16.750146Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Training data saved to marinedata_train.pkl\nTest data saved to marinedata_test.pkl\nValidation data saved to marinedata_val.pkl\n","output_type":"stream"}]}]}