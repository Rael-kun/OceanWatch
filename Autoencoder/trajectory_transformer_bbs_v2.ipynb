{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First implementation of a trajectory transformer, where each observation is a vector of [lat lon, sog, cog]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from typing import Iterator, List\n",
    "\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as f\n",
    "import torch.nn.functional as F\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from tqdm.notebook import tqdm as tnb\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (could move to config file if wanna be fancy)\n",
    "\n",
    "train_path = './Data/OG Data/marinedata_train.pkl'\n",
    "val_path = './Data/OG Data/marinedata_val.pkl'\n",
    "\n",
    "d_model = 128 # Size of the embedding space\n",
    "max_seq_time = 144 # Maximum number of observations per trajectory\n",
    "min_seq_time = 24\n",
    "input_dim = 4 # [lat, lon, sog, cog]\n",
    "output_dim = input_dim # [lat, lon, sog, cog]\n",
    "n_enc_layers = 6 \n",
    "n_dec_layers = n_enc_layers\n",
    "n_heads = 8 # number of attentionheads in the encoder and decoder\n",
    "d_ff = 512 # Dimension of the ff nn at the end of a block\n",
    "\n",
    "batch_by_size = True\n",
    "batch_size = 64\n",
    "\n",
    "params = [input_dim, d_model, n_heads, n_enc_layers, n_dec_layers, d_ff, max_seq_time, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Postional encoder using the sinusiodals from the original paper\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype = torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-torch.log(torch.tensor(10000.0))/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        self.pe = pe.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TrajectoryEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Encoder relying mainly on pytorchs modules \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model, n_heads, n_enc_layers, d_ff, max_len):\n",
    "        super(TrajectoryEncoder, self).__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        enc_layers = nn.TransformerEncoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(enc_layers, n_enc_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = self.input_proj(src) # Take our input from input_dim to embedding-d\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, n_heads, n_dec_layers, d_ff, max_len, \n",
    "                 output_dim):\n",
    "        super(TrajectoryDecoder, self).__init__()\n",
    "        self.emb_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        dec_layers = nn.TransformerDecoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(dec_layers, n_dec_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        \"\"\"\n",
    "        tgt (Tensor) = the sequence to the decoder (required).\n",
    "\n",
    "        memory (Tensor) = the sequence from the last layer of the encoder. Size bs, T, n_emb\n",
    "        \"\"\"\n",
    "        tgt = self.emb_proj(tgt)\n",
    "        target = self.pos_enc(tgt)\n",
    "        decoded = self.transformer_decoder(target, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "        output = self.output_proj(decoded) # project down to output_dim\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads, n_enc_layers, n_dec_layers, d_ff, max_len, output_dim):\n",
    "        super(TrajectoryAutoencoder, self).__init__()\n",
    "        self.encoder = TrajectoryEncoder(input_dim, d_model, n_heads, n_enc_layers, d_ff, max_len)\n",
    "        self.decoder = TrajectoryDecoder(input_dim, d_model, n_heads, n_dec_layers, d_ff, max_len, output_dim)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        memory = self.encoder(tgt)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1))\n",
    "        reconstructed = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time = min_seq_time\n",
    "if min_time>max_seq_time:\n",
    "    min_time = max_seq_time\n",
    "\n",
    "class MarineDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.data[idx]).to(device)\n",
    "\n",
    "class MarineDatasetPadding(Dataset):\n",
    "    def __init__(self, data_dictionary, min_seq_len, max_seq_len, n_features):\n",
    "\n",
    "        self.min_seq_len = min_seq_len\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.dict = data_dictionary\n",
    "        self.keys = list(data_dictionary.keys())\n",
    "\n",
    "        for key in self.keys:\n",
    "            if len(self.dict[key]['traj']) < min_seq_len:\n",
    "                del self.dict[key]\n",
    "                self.keys.remove(key)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        traj_data = self.dict[key]['traj']\n",
    "        \n",
    "        # Cut off the mmsi and timestamp and return trajectory\n",
    "        if len(traj_data) >= self.max_seq_len:\n",
    "            return torch.tensor(traj_data[:self.max_seq_len,:self.n_features])\n",
    "        else: \n",
    "            needed_padding = self.max_seq_len - len(traj_data)\n",
    "            res = F.pad(torch.tensor(traj_data[:,:self.n_features]), (0,0, 0, needed_padding), \"constant\", 0.0)\n",
    "        \n",
    "            return res.to(device)\n",
    "\n",
    "class BatchBySizeSampler(Sampler[List[int]]):\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        paths_by_length = defaultdict(list)\n",
    "        for i, path in enumerate(data):\n",
    "            paths_by_length[len(path)].append(i)\n",
    "        self.paths_by_length = list(paths_by_length.values())\n",
    "        self.num_batches = sum((len(xs) + batch_size - 1) // batch_size for xs in self.paths_by_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        for xs in self.paths_by_length:\n",
    "            np.random.shuffle(xs)\n",
    "        chunks = [xs[i : i + self.batch_size] for xs in self.paths_by_length for i in range(0, len(xs), self.batch_size)]\n",
    "        np.random.shuffle(chunks)\n",
    "        yield from chunks\n",
    "\n",
    "\n",
    "def merge_path_lists(l1, l2):\n",
    "    inner_1 = len(l1[0])\n",
    "    inner_2 = len(l2[0])\n",
    "    if inner_1 > inner_2:\n",
    "        l1, l2 = l2, l1\n",
    "    target_inner = len(l1[0])\n",
    "    for p in l2:\n",
    "        l1.append(p[:target_inner])\n",
    "    return l1\n",
    "\n",
    "def merge_lengths(all_paths):\n",
    "    paths_by_length = defaultdict(list)\n",
    "    for path in all_paths:\n",
    "        paths_by_length[len(path)].append(path)\n",
    "    paths_by_length = list(paths_by_length.values())\n",
    "    paths_by_length.sort(key=len)\n",
    "    fixed = []\n",
    "    i = 0\n",
    "    while i < len(paths_by_length):\n",
    "        curr = paths_by_length[i]\n",
    "        \"\"\"\n",
    "        maybe make each bucket strictly larger than batch size (currently equal is allowed)?\n",
    "         - this would prevent batches with exact same items between epochs\n",
    "         - problem is it would lead to small \"excess batches\", that is batches with fewer than batch_size #paths in them\n",
    "         - for example, bucket of size (batch_size + 1) leads to one batch with batch_size #paths and one with a single path\n",
    "                => might mess with gradient step\n",
    "        \"\"\"\n",
    "        while len(curr) < batch_size:\n",
    "            curr = merge_path_lists(curr, paths_by_length[i + 1])\n",
    "            i += 1\n",
    "        fixed.append(curr)\n",
    "        i += 1\n",
    "    return [x for xs in fixed for x in xs]\n",
    "\n",
    "\n",
    "if batch_by_size:\n",
    "    with open(train_path, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "        train_data = [x[\"traj\"][:max_seq_time,:input_dim] for x in train_data.values() if len(x[\"traj\"]) >= min_seq_time]\n",
    "    with open(val_path, 'rb') as f:\n",
    "        val_data = pickle.load(f)\n",
    "        val_data = [x[\"traj\"][:max_seq_time,:input_dim] for x in val_data.values() if len(x[\"traj\"]) >= min_seq_time]\n",
    "    \n",
    "    train_merged = merge_lengths(train_data)\n",
    "    val_merged = merge_lengths(val_data)\n",
    "    \n",
    "    train_dataloader = DataLoader(MarineDataset(train_merged), batch_sampler=BatchBySizeSampler(train_merged, batch_size))\n",
    "    validation_dataloader = DataLoader(MarineDataset(val_merged), batch_sampler=BatchBySizeSampler(val_merged, batch_size))\n",
    "else:\n",
    "    with open(train_path, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(val_path, 'rb') as f:\n",
    "        val_data = pickle.load(f)\n",
    "    train_dataloader = DataLoader(MarineDatasetPadding(train_data, min_seq_len=min_seq_time, max_seq_len=max_seq_time, n_features=input_dim), batch_size=batch_size, shuffle=True)\n",
    "    validation_dataloader = DataLoader(MarineDatasetPadding(val_data, min_seq_len=min_seq_time, max_seq_len=max_seq_time, n_features=input_dim), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lat min 53.4 max 66.2\n",
    "# lon min 9.4 max 30.5\n",
    "# Max speed 50\n",
    "# Max heading 360\n",
    "\n",
    "def train_loop(model, dataloader, criterion, optimizer):\n",
    "    train_data_size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    cum_loss = 0.0\n",
    "    acc = 0\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "        acc += len(X)\n",
    "        if batch % 10 == 0:\n",
    "            print(f' loss {loss.item():>4f} [{acc:>5d} / {train_data_size:>5d} ]', flush=True)\n",
    "    return cum_loss\n",
    "\n",
    "def test_loop(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    cum_loss = 0.0\n",
    "    loss = []\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X)\n",
    "            current_loss = criterion(X, pred).item()\n",
    "            cum_loss += current_loss\n",
    "\n",
    "            loss.append(current_loss)\n",
    "\n",
    "    print(f'Test set: \\nCumulative loss: {cum_loss}, Average Loss {np.mean(loss):>8f}')\n",
    "    return cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrajectoryAutoencoder(*params)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuck you; It's epoch #1\n",
      " loss 0.613542 [   64 / 11478 ]\n",
      " loss 0.347296 [  613 / 11478 ]\n",
      " loss 0.078815 [ 1134 / 11478 ]\n",
      " loss 0.153601 [ 1604 / 11478 ]\n",
      " loss 0.145908 [ 2112 / 11478 ]\n",
      " loss 0.118089 [ 2590 / 11478 ]\n",
      " loss 0.105690 [ 3049 / 11478 ]\n",
      " loss 0.108585 [ 3552 / 11478 ]\n",
      " loss 0.090021 [ 4023 / 11478 ]\n",
      " loss 0.091711 [ 4566 / 11478 ]\n",
      " loss 0.100571 [ 5060 / 11478 ]\n",
      " loss 0.082540 [ 5620 / 11478 ]\n",
      " loss 0.076392 [ 6119 / 11478 ]\n",
      " loss 0.077400 [ 6612 / 11478 ]\n",
      " loss 0.067198 [ 7165 / 11478 ]\n",
      " loss 0.071464 [ 7615 / 11478 ]\n",
      " loss 0.066108 [ 8103 / 11478 ]\n",
      " loss 0.063035 [ 8660 / 11478 ]\n",
      " loss 0.060404 [ 9100 / 11478 ]\n",
      " loss 0.059601 [ 9510 / 11478 ]\n",
      " loss 0.051111 [ 9992 / 11478 ]\n",
      " loss 0.060825 [10434 / 11478 ]\n",
      " loss 0.061473 [10893 / 11478 ]\n",
      " loss 0.051518 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 9.512632821025802, Average Loss 0.041540\n",
      "Fuck you; It's epoch #2\n",
      " loss 0.053930 [   64 / 11478 ]\n",
      " loss 0.050740 [  613 / 11478 ]\n",
      " loss 0.043979 [ 1134 / 11478 ]\n",
      " loss 0.047813 [ 1604 / 11478 ]\n",
      " loss 0.050837 [ 2112 / 11478 ]\n",
      " loss 0.044712 [ 2590 / 11478 ]\n",
      " loss 0.049571 [ 3049 / 11478 ]\n",
      " loss 0.047222 [ 3552 / 11478 ]\n",
      " loss 0.041476 [ 4023 / 11478 ]\n",
      " loss 0.044045 [ 4566 / 11478 ]\n",
      " loss 0.044134 [ 5060 / 11478 ]\n",
      " loss 0.042441 [ 5620 / 11478 ]\n",
      " loss 0.044134 [ 6119 / 11478 ]\n",
      " loss 0.043656 [ 6612 / 11478 ]\n",
      " loss 0.037793 [ 7165 / 11478 ]\n",
      " loss 0.038929 [ 7615 / 11478 ]\n",
      " loss 0.036129 [ 8103 / 11478 ]\n",
      " loss 0.035351 [ 8660 / 11478 ]\n",
      " loss 0.037108 [ 9100 / 11478 ]\n",
      " loss 0.036273 [ 9510 / 11478 ]\n",
      " loss 0.031684 [ 9992 / 11478 ]\n",
      " loss 0.036754 [10434 / 11478 ]\n",
      " loss 0.033559 [10893 / 11478 ]\n",
      " loss 0.032835 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 4.66117260643714, Average Loss 0.020354\n",
      "Fuck you; It's epoch #3\n",
      " loss 0.035228 [   64 / 11478 ]\n",
      " loss 0.027857 [  613 / 11478 ]\n",
      " loss 0.022724 [ 1134 / 11478 ]\n",
      " loss 0.030890 [ 1604 / 11478 ]\n",
      " loss 0.030879 [ 2112 / 11478 ]\n",
      " loss 0.029076 [ 2590 / 11478 ]\n",
      " loss 0.027769 [ 3049 / 11478 ]\n",
      " loss 0.027780 [ 3552 / 11478 ]\n",
      " loss 0.026516 [ 4023 / 11478 ]\n",
      " loss 0.029871 [ 4566 / 11478 ]\n",
      " loss 0.027903 [ 5060 / 11478 ]\n",
      " loss 0.029152 [ 5620 / 11478 ]\n",
      " loss 0.026248 [ 6119 / 11478 ]\n",
      " loss 0.027872 [ 6612 / 11478 ]\n",
      " loss 0.024801 [ 7165 / 11478 ]\n",
      " loss 0.026946 [ 7615 / 11478 ]\n",
      " loss 0.025239 [ 8103 / 11478 ]\n",
      " loss 0.023582 [ 8660 / 11478 ]\n",
      " loss 0.022911 [ 9100 / 11478 ]\n",
      " loss 0.026126 [ 9510 / 11478 ]\n",
      " loss 0.023203 [ 9992 / 11478 ]\n",
      " loss 0.025142 [10434 / 11478 ]\n",
      " loss 0.020157 [10893 / 11478 ]\n",
      " loss 0.023373 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 2.886366600465398, Average Loss 0.012604\n",
      "Fuck you; It's epoch #4\n",
      " loss 0.024242 [   64 / 11478 ]\n",
      " loss 0.022286 [  613 / 11478 ]\n",
      " loss 0.021832 [ 1134 / 11478 ]\n",
      " loss 0.023346 [ 1604 / 11478 ]\n",
      " loss 0.024494 [ 2112 / 11478 ]\n",
      " loss 0.021960 [ 2590 / 11478 ]\n",
      " loss 0.018898 [ 3049 / 11478 ]\n",
      " loss 0.019713 [ 3552 / 11478 ]\n",
      " loss 0.018276 [ 4023 / 11478 ]\n",
      " loss 0.021770 [ 4566 / 11478 ]\n",
      " loss 0.019834 [ 5060 / 11478 ]\n",
      " loss 0.023771 [ 5620 / 11478 ]\n",
      " loss 0.021338 [ 6119 / 11478 ]\n",
      " loss 0.021298 [ 6612 / 11478 ]\n",
      " loss 0.019888 [ 7165 / 11478 ]\n",
      " loss 0.021671 [ 7615 / 11478 ]\n",
      " loss 0.018295 [ 8103 / 11478 ]\n",
      " loss 0.019587 [ 8660 / 11478 ]\n",
      " loss 0.018743 [ 9100 / 11478 ]\n",
      " loss 0.020646 [ 9510 / 11478 ]\n",
      " loss 0.017503 [ 9992 / 11478 ]\n",
      " loss 0.019259 [10434 / 11478 ]\n",
      " loss 0.015578 [10893 / 11478 ]\n",
      " loss 0.018484 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 2.1680297292672206, Average Loss 0.009467\n",
      "Fuck you; It's epoch #5\n",
      " loss 0.017827 [   64 / 11478 ]\n",
      " loss 0.016732 [  613 / 11478 ]\n",
      " loss 0.019024 [ 1134 / 11478 ]\n",
      " loss 0.018041 [ 1604 / 11478 ]\n",
      " loss 0.017744 [ 2112 / 11478 ]\n",
      " loss 0.016968 [ 2590 / 11478 ]\n",
      " loss 0.015344 [ 3049 / 11478 ]\n",
      " loss 0.015545 [ 3552 / 11478 ]\n",
      " loss 0.014389 [ 4023 / 11478 ]\n",
      " loss 0.015462 [ 4566 / 11478 ]\n",
      " loss 0.014996 [ 5060 / 11478 ]\n",
      " loss 0.016289 [ 5620 / 11478 ]\n",
      " loss 0.015206 [ 6119 / 11478 ]\n",
      " loss 0.015612 [ 6612 / 11478 ]\n",
      " loss 0.014468 [ 7165 / 11478 ]\n",
      " loss 0.015286 [ 7615 / 11478 ]\n",
      " loss 0.012686 [ 8103 / 11478 ]\n",
      " loss 0.014171 [ 8660 / 11478 ]\n",
      " loss 0.013437 [ 9100 / 11478 ]\n",
      " loss 0.013906 [ 9510 / 11478 ]\n",
      " loss 0.012233 [ 9992 / 11478 ]\n",
      " loss 0.013140 [10434 / 11478 ]\n",
      " loss 0.011750 [10893 / 11478 ]\n",
      " loss 0.012441 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 1.1141734761830486, Average Loss 0.004865\n",
      "Fuck you; It's epoch #6\n",
      " loss 0.012465 [   64 / 11478 ]\n",
      " loss 0.012125 [  613 / 11478 ]\n",
      " loss 0.010162 [ 1134 / 11478 ]\n",
      " loss 0.012026 [ 1604 / 11478 ]\n",
      " loss 0.011611 [ 2112 / 11478 ]\n",
      " loss 0.011511 [ 2590 / 11478 ]\n",
      " loss 0.010839 [ 3049 / 11478 ]\n",
      " loss 0.011162 [ 3552 / 11478 ]\n",
      " loss 0.010462 [ 4023 / 11478 ]\n",
      " loss 0.010961 [ 4566 / 11478 ]\n",
      " loss 0.011254 [ 5060 / 11478 ]\n",
      " loss 0.010327 [ 5620 / 11478 ]\n",
      " loss 0.009683 [ 6119 / 11478 ]\n",
      " loss 0.009354 [ 6612 / 11478 ]\n",
      " loss 0.009335 [ 7165 / 11478 ]\n",
      " loss 0.009672 [ 7615 / 11478 ]\n",
      " loss 0.009242 [ 8103 / 11478 ]\n",
      " loss 0.009267 [ 8660 / 11478 ]\n",
      " loss 0.008951 [ 9100 / 11478 ]\n",
      " loss 0.008905 [ 9510 / 11478 ]\n",
      " loss 0.008521 [ 9992 / 11478 ]\n",
      " loss 0.009332 [10434 / 11478 ]\n",
      " loss 0.008385 [10893 / 11478 ]\n",
      " loss 0.008140 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.633526744849655, Average Loss 0.002766\n",
      "Fuck you; It's epoch #7\n",
      " loss 0.008635 [   64 / 11478 ]\n",
      " loss 0.008140 [  613 / 11478 ]\n",
      " loss 0.007937 [ 1134 / 11478 ]\n",
      " loss 0.008572 [ 1604 / 11478 ]\n",
      " loss 0.008297 [ 2112 / 11478 ]\n",
      " loss 0.008198 [ 2590 / 11478 ]\n",
      " loss 0.007728 [ 3049 / 11478 ]\n",
      " loss 0.007961 [ 3552 / 11478 ]\n",
      " loss 0.007843 [ 4023 / 11478 ]\n",
      " loss 0.008092 [ 4566 / 11478 ]\n",
      " loss 0.007884 [ 5060 / 11478 ]\n",
      " loss 0.007424 [ 5620 / 11478 ]\n",
      " loss 0.007315 [ 6119 / 11478 ]\n",
      " loss 0.007338 [ 6612 / 11478 ]\n",
      " loss 0.007640 [ 7165 / 11478 ]\n",
      " loss 0.007270 [ 7615 / 11478 ]\n",
      " loss 0.007371 [ 8103 / 11478 ]\n",
      " loss 0.007294 [ 8660 / 11478 ]\n",
      " loss 0.007007 [ 9100 / 11478 ]\n",
      " loss 0.006672 [ 9510 / 11478 ]\n",
      " loss 0.006734 [ 9992 / 11478 ]\n",
      " loss 0.007014 [10434 / 11478 ]\n",
      " loss 0.006336 [10893 / 11478 ]\n",
      " loss 0.006515 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.556272042732785, Average Loss 0.002429\n",
      "Fuck you; It's epoch #8\n",
      " loss 0.006858 [   64 / 11478 ]\n",
      " loss 0.006739 [  613 / 11478 ]\n",
      " loss 0.005671 [ 1134 / 11478 ]\n",
      " loss 0.006738 [ 1604 / 11478 ]\n",
      " loss 0.006753 [ 2112 / 11478 ]\n",
      " loss 0.006551 [ 2590 / 11478 ]\n",
      " loss 0.006262 [ 3049 / 11478 ]\n",
      " loss 0.006622 [ 3552 / 11478 ]\n",
      " loss 0.006090 [ 4023 / 11478 ]\n",
      " loss 0.006878 [ 4566 / 11478 ]\n",
      " loss 0.006561 [ 5060 / 11478 ]\n",
      " loss 0.006174 [ 5620 / 11478 ]\n",
      " loss 0.006019 [ 6119 / 11478 ]\n",
      " loss 0.005703 [ 6612 / 11478 ]\n",
      " loss 0.005925 [ 7165 / 11478 ]\n",
      " loss 0.005682 [ 7615 / 11478 ]\n",
      " loss 0.005586 [ 8103 / 11478 ]\n",
      " loss 0.005780 [ 8660 / 11478 ]\n",
      " loss 0.005963 [ 9100 / 11478 ]\n",
      " loss 0.005617 [ 9510 / 11478 ]\n",
      " loss 0.005413 [ 9992 / 11478 ]\n",
      " loss 0.005909 [10434 / 11478 ]\n",
      " loss 0.005027 [10893 / 11478 ]\n",
      " loss 0.005038 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.5117872375881238, Average Loss 0.002235\n",
      "Fuck you; It's epoch #9\n",
      " loss 0.005191 [   64 / 11478 ]\n",
      " loss 0.005280 [  613 / 11478 ]\n",
      " loss 0.005601 [ 1134 / 11478 ]\n",
      " loss 0.005473 [ 1604 / 11478 ]\n",
      " loss 0.005444 [ 2112 / 11478 ]\n",
      " loss 0.005358 [ 2590 / 11478 ]\n",
      " loss 0.005261 [ 3049 / 11478 ]\n",
      " loss 0.005421 [ 3552 / 11478 ]\n",
      " loss 0.004841 [ 4023 / 11478 ]\n",
      " loss 0.005276 [ 4566 / 11478 ]\n",
      " loss 0.005233 [ 5060 / 11478 ]\n",
      " loss 0.004930 [ 5620 / 11478 ]\n",
      " loss 0.004795 [ 6119 / 11478 ]\n",
      " loss 0.004718 [ 6612 / 11478 ]\n",
      " loss 0.004782 [ 7165 / 11478 ]\n",
      " loss 0.004649 [ 7615 / 11478 ]\n",
      " loss 0.004857 [ 8103 / 11478 ]\n",
      " loss 0.004904 [ 8660 / 11478 ]\n",
      " loss 0.004972 [ 9100 / 11478 ]\n",
      " loss 0.004466 [ 9510 / 11478 ]\n",
      " loss 0.004388 [ 9992 / 11478 ]\n",
      " loss 0.005404 [10434 / 11478 ]\n",
      " loss 0.004120 [10893 / 11478 ]\n",
      " loss 0.004295 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.5500597873260517, Average Loss 0.002402\n",
      "Fuck you; It's epoch #10\n",
      " loss 0.004642 [   64 / 11478 ]\n",
      " loss 0.004426 [  613 / 11478 ]\n",
      " loss 0.004244 [ 1134 / 11478 ]\n",
      " loss 0.004544 [ 1604 / 11478 ]\n",
      " loss 0.004628 [ 2112 / 11478 ]\n",
      " loss 0.004476 [ 2590 / 11478 ]\n",
      " loss 0.004734 [ 3049 / 11478 ]\n",
      " loss 0.005078 [ 3552 / 11478 ]\n",
      " loss 0.003850 [ 4023 / 11478 ]\n",
      " loss 0.004382 [ 4566 / 11478 ]\n",
      " loss 0.004473 [ 5060 / 11478 ]\n",
      " loss 0.004059 [ 5620 / 11478 ]\n",
      " loss 0.003784 [ 6119 / 11478 ]\n",
      " loss 0.004008 [ 6612 / 11478 ]\n",
      " loss 0.004295 [ 7165 / 11478 ]\n",
      " loss 0.004083 [ 7615 / 11478 ]\n",
      " loss 0.004061 [ 8103 / 11478 ]\n",
      " loss 0.004182 [ 8660 / 11478 ]\n",
      " loss 0.003874 [ 9100 / 11478 ]\n",
      " loss 0.003793 [ 9510 / 11478 ]\n",
      " loss 0.003901 [ 9992 / 11478 ]\n",
      " loss 0.004481 [10434 / 11478 ]\n",
      " loss 0.003716 [10893 / 11478 ]\n",
      " loss 0.003443 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.441167699613678, Average Loss 0.001926\n",
      "Fuck you; It's epoch #11\n",
      " loss 0.003798 [   64 / 11478 ]\n",
      " loss 0.003584 [  613 / 11478 ]\n",
      " loss 0.003429 [ 1134 / 11478 ]\n",
      " loss 0.003811 [ 1604 / 11478 ]\n",
      " loss 0.003801 [ 2112 / 11478 ]\n",
      " loss 0.003896 [ 2590 / 11478 ]\n",
      " loss 0.003943 [ 3049 / 11478 ]\n",
      " loss 0.003948 [ 3552 / 11478 ]\n",
      " loss 0.003467 [ 4023 / 11478 ]\n",
      " loss 0.003896 [ 4566 / 11478 ]\n",
      " loss 0.003457 [ 5060 / 11478 ]\n",
      " loss 0.003540 [ 5620 / 11478 ]\n",
      " loss 0.003355 [ 6119 / 11478 ]\n",
      " loss 0.003411 [ 6612 / 11478 ]\n",
      " loss 0.003433 [ 7165 / 11478 ]\n",
      " loss 0.003529 [ 7615 / 11478 ]\n",
      " loss 0.003452 [ 8103 / 11478 ]\n",
      " loss 0.003521 [ 8660 / 11478 ]\n",
      " loss 0.003767 [ 9100 / 11478 ]\n",
      " loss 0.003279 [ 9510 / 11478 ]\n",
      " loss 0.003415 [ 9992 / 11478 ]\n",
      " loss 0.004143 [10434 / 11478 ]\n",
      " loss 0.003759 [10893 / 11478 ]\n",
      " loss 0.003162 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.4636454331516222, Average Loss 0.002025\n",
      "Fuck you; It's epoch #12\n",
      " loss 0.003395 [   64 / 11478 ]\n",
      " loss 0.002913 [  613 / 11478 ]\n",
      " loss 0.002860 [ 1134 / 11478 ]\n",
      " loss 0.003415 [ 1604 / 11478 ]\n",
      " loss 0.003582 [ 2112 / 11478 ]\n",
      " loss 0.003441 [ 2590 / 11478 ]\n",
      " loss 0.002927 [ 3049 / 11478 ]\n",
      " loss 0.003348 [ 3552 / 11478 ]\n",
      " loss 0.002904 [ 4023 / 11478 ]\n",
      " loss 0.003542 [ 4566 / 11478 ]\n",
      " loss 0.003205 [ 5060 / 11478 ]\n",
      " loss 0.003059 [ 5620 / 11478 ]\n",
      " loss 0.002918 [ 6119 / 11478 ]\n",
      " loss 0.002879 [ 6612 / 11478 ]\n",
      " loss 0.002888 [ 7165 / 11478 ]\n",
      " loss 0.002827 [ 7615 / 11478 ]\n",
      " loss 0.002774 [ 8103 / 11478 ]\n",
      " loss 0.002998 [ 8660 / 11478 ]\n",
      " loss 0.003200 [ 9100 / 11478 ]\n",
      " loss 0.002767 [ 9510 / 11478 ]\n",
      " loss 0.002716 [ 9992 / 11478 ]\n",
      " loss 0.002995 [10434 / 11478 ]\n",
      " loss 0.003132 [10893 / 11478 ]\n",
      " loss 0.003910 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.5966439239339448, Average Loss 0.002605\n",
      "Fuck you; It's epoch #13\n",
      " loss 0.003407 [   64 / 11478 ]\n",
      " loss 0.002792 [  613 / 11478 ]\n",
      " loss 0.002777 [ 1134 / 11478 ]\n",
      " loss 0.003021 [ 1604 / 11478 ]\n",
      " loss 0.003144 [ 2112 / 11478 ]\n",
      " loss 0.002875 [ 2590 / 11478 ]\n",
      " loss 0.002855 [ 3049 / 11478 ]\n",
      " loss 0.004322 [ 3552 / 11478 ]\n",
      " loss 0.002932 [ 4023 / 11478 ]\n",
      " loss 0.003085 [ 4566 / 11478 ]\n",
      " loss 0.002987 [ 5060 / 11478 ]\n",
      " loss 0.002750 [ 5620 / 11478 ]\n",
      " loss 0.002593 [ 6119 / 11478 ]\n",
      " loss 0.002564 [ 6612 / 11478 ]\n",
      " loss 0.002501 [ 7165 / 11478 ]\n",
      " loss 0.002645 [ 7615 / 11478 ]\n",
      " loss 0.002494 [ 8103 / 11478 ]\n",
      " loss 0.002665 [ 8660 / 11478 ]\n",
      " loss 0.002903 [ 9100 / 11478 ]\n",
      " loss 0.002526 [ 9510 / 11478 ]\n",
      " loss 0.002666 [ 9992 / 11478 ]\n",
      " loss 0.002986 [10434 / 11478 ]\n",
      " loss 0.002884 [10893 / 11478 ]\n",
      " loss 0.002615 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.46010413482889434, Average Loss 0.002009\n",
      "Fuck you; It's epoch #14\n",
      " loss 0.002707 [   64 / 11478 ]\n",
      " loss 0.002263 [  613 / 11478 ]\n",
      " loss 0.002600 [ 1134 / 11478 ]\n",
      " loss 0.002551 [ 1604 / 11478 ]\n",
      " loss 0.002762 [ 2112 / 11478 ]\n",
      " loss 0.002541 [ 2590 / 11478 ]\n",
      " loss 0.002319 [ 3049 / 11478 ]\n",
      " loss 0.002926 [ 3552 / 11478 ]\n",
      " loss 0.002207 [ 4023 / 11478 ]\n",
      " loss 0.002954 [ 4566 / 11478 ]\n",
      " loss 0.002437 [ 5060 / 11478 ]\n",
      " loss 0.002367 [ 5620 / 11478 ]\n",
      " loss 0.002213 [ 6119 / 11478 ]\n",
      " loss 0.002321 [ 6612 / 11478 ]\n",
      " loss 0.002350 [ 7165 / 11478 ]\n",
      " loss 0.002426 [ 7615 / 11478 ]\n",
      " loss 0.002328 [ 8103 / 11478 ]\n",
      " loss 0.002317 [ 8660 / 11478 ]\n",
      " loss 0.002640 [ 9100 / 11478 ]\n",
      " loss 0.002281 [ 9510 / 11478 ]\n",
      " loss 0.002420 [ 9992 / 11478 ]\n",
      " loss 0.002551 [10434 / 11478 ]\n",
      " loss 0.002588 [10893 / 11478 ]\n",
      " loss 0.002420 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.4374533076693281, Average Loss 0.001910\n",
      "Fuck you; It's epoch #15\n",
      " loss 0.002224 [   64 / 11478 ]\n",
      " loss 0.001998 [  613 / 11478 ]\n",
      " loss 0.002010 [ 1134 / 11478 ]\n",
      " loss 0.002367 [ 1604 / 11478 ]\n",
      " loss 0.002438 [ 2112 / 11478 ]\n",
      " loss 0.002358 [ 2590 / 11478 ]\n",
      " loss 0.002009 [ 3049 / 11478 ]\n",
      " loss 0.002328 [ 3552 / 11478 ]\n",
      " loss 0.002019 [ 4023 / 11478 ]\n",
      " loss 0.002562 [ 4566 / 11478 ]\n",
      " loss 0.002306 [ 5060 / 11478 ]\n",
      " loss 0.002275 [ 5620 / 11478 ]\n",
      " loss 0.001873 [ 6119 / 11478 ]\n",
      " loss 0.002223 [ 6612 / 11478 ]\n",
      " loss 0.001942 [ 7165 / 11478 ]\n",
      " loss 0.002061 [ 7615 / 11478 ]\n",
      " loss 0.001952 [ 8103 / 11478 ]\n",
      " loss 0.002154 [ 8660 / 11478 ]\n",
      " loss 0.002457 [ 9100 / 11478 ]\n",
      " loss 0.001992 [ 9510 / 11478 ]\n",
      " loss 0.002129 [ 9992 / 11478 ]\n",
      " loss 0.002338 [10434 / 11478 ]\n",
      " loss 0.001933 [10893 / 11478 ]\n",
      " loss 0.002078 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.38186009631546664, Average Loss 0.001668\n",
      "Fuck you; It's epoch #16\n",
      " loss 0.002049 [   64 / 11478 ]\n",
      " loss 0.001847 [  613 / 11478 ]\n",
      " loss 0.002154 [ 1134 / 11478 ]\n",
      " loss 0.002191 [ 1604 / 11478 ]\n",
      " loss 0.002538 [ 2112 / 11478 ]\n",
      " loss 0.002064 [ 2590 / 11478 ]\n",
      " loss 0.001689 [ 3049 / 11478 ]\n",
      " loss 0.002029 [ 3552 / 11478 ]\n",
      " loss 0.001939 [ 4023 / 11478 ]\n",
      " loss 0.002537 [ 4566 / 11478 ]\n",
      " loss 0.001899 [ 5060 / 11478 ]\n",
      " loss 0.002074 [ 5620 / 11478 ]\n",
      " loss 0.001746 [ 6119 / 11478 ]\n",
      " loss 0.001994 [ 6612 / 11478 ]\n",
      " loss 0.001722 [ 7165 / 11478 ]\n",
      " loss 0.001921 [ 7615 / 11478 ]\n",
      " loss 0.001740 [ 8103 / 11478 ]\n",
      " loss 0.002078 [ 8660 / 11478 ]\n",
      " loss 0.002123 [ 9100 / 11478 ]\n",
      " loss 0.001981 [ 9510 / 11478 ]\n",
      " loss 0.002009 [ 9992 / 11478 ]\n",
      " loss 0.002457 [10434 / 11478 ]\n",
      " loss 0.001684 [10893 / 11478 ]\n",
      " loss 0.001940 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.4314710868641672, Average Loss 0.001884\n",
      "Fuck you; It's epoch #17\n",
      " loss 0.002059 [   64 / 11478 ]\n",
      " loss 0.001786 [  613 / 11478 ]\n",
      " loss 0.002137 [ 1134 / 11478 ]\n",
      " loss 0.002034 [ 1604 / 11478 ]\n",
      " loss 0.002254 [ 2112 / 11478 ]\n",
      " loss 0.002012 [ 2590 / 11478 ]\n",
      " loss 0.001904 [ 3049 / 11478 ]\n",
      " loss 0.004339 [ 3552 / 11478 ]\n",
      " loss 0.002500 [ 4023 / 11478 ]\n",
      " loss 0.002352 [ 4566 / 11478 ]\n",
      " loss 0.002136 [ 5060 / 11478 ]\n",
      " loss 0.001836 [ 5620 / 11478 ]\n",
      " loss 0.001686 [ 6119 / 11478 ]\n",
      " loss 0.001748 [ 6612 / 11478 ]\n",
      " loss 0.001747 [ 7165 / 11478 ]\n",
      " loss 0.001800 [ 7615 / 11478 ]\n",
      " loss 0.001614 [ 8103 / 11478 ]\n",
      " loss 0.001814 [ 8660 / 11478 ]\n",
      " loss 0.002055 [ 9100 / 11478 ]\n",
      " loss 0.001631 [ 9510 / 11478 ]\n",
      " loss 0.001909 [ 9992 / 11478 ]\n",
      " loss 0.002113 [10434 / 11478 ]\n",
      " loss 0.001766 [10893 / 11478 ]\n",
      " loss 0.001820 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.39878572964077286, Average Loss 0.001741\n",
      "Fuck you; It's epoch #18\n",
      " loss 0.001812 [   64 / 11478 ]\n",
      " loss 0.001554 [  613 / 11478 ]\n",
      " loss 0.001468 [ 1134 / 11478 ]\n",
      " loss 0.001709 [ 1604 / 11478 ]\n",
      " loss 0.001979 [ 2112 / 11478 ]\n",
      " loss 0.001786 [ 2590 / 11478 ]\n",
      " loss 0.001492 [ 3049 / 11478 ]\n",
      " loss 0.001533 [ 3552 / 11478 ]\n",
      " loss 0.001406 [ 4023 / 11478 ]\n",
      " loss 0.002009 [ 4566 / 11478 ]\n",
      " loss 0.002056 [ 5060 / 11478 ]\n",
      " loss 0.001734 [ 5620 / 11478 ]\n",
      " loss 0.001521 [ 6119 / 11478 ]\n",
      " loss 0.001610 [ 6612 / 11478 ]\n",
      " loss 0.001569 [ 7165 / 11478 ]\n",
      " loss 0.001747 [ 7615 / 11478 ]\n",
      " loss 0.001642 [ 8103 / 11478 ]\n",
      " loss 0.001747 [ 8660 / 11478 ]\n",
      " loss 0.001906 [ 9100 / 11478 ]\n",
      " loss 0.001538 [ 9510 / 11478 ]\n",
      " loss 0.001513 [ 9992 / 11478 ]\n",
      " loss 0.001889 [10434 / 11478 ]\n",
      " loss 0.001806 [10893 / 11478 ]\n",
      " loss 0.001410 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.3305771766185146, Average Loss 0.001444\n",
      "Fuck you; It's epoch #19\n",
      " loss 0.001443 [   64 / 11478 ]\n",
      " loss 0.001244 [  613 / 11478 ]\n",
      " loss 0.001742 [ 1134 / 11478 ]\n",
      " loss 0.001546 [ 1604 / 11478 ]\n",
      " loss 0.001717 [ 2112 / 11478 ]\n",
      " loss 0.001590 [ 2590 / 11478 ]\n",
      " loss 0.001398 [ 3049 / 11478 ]\n",
      " loss 0.001864 [ 3552 / 11478 ]\n",
      " loss 0.001725 [ 4023 / 11478 ]\n",
      " loss 0.001994 [ 4566 / 11478 ]\n",
      " loss 0.001574 [ 5060 / 11478 ]\n",
      " loss 0.001643 [ 5620 / 11478 ]\n",
      " loss 0.001353 [ 6119 / 11478 ]\n",
      " loss 0.001447 [ 6612 / 11478 ]\n",
      " loss 0.001403 [ 7165 / 11478 ]\n",
      " loss 0.001495 [ 7615 / 11478 ]\n",
      " loss 0.001313 [ 8103 / 11478 ]\n",
      " loss 0.001458 [ 8660 / 11478 ]\n",
      " loss 0.001736 [ 9100 / 11478 ]\n",
      " loss 0.001577 [ 9510 / 11478 ]\n",
      " loss 0.001365 [ 9992 / 11478 ]\n",
      " loss 0.001697 [10434 / 11478 ]\n",
      " loss 0.001300 [10893 / 11478 ]\n",
      " loss 0.001306 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.3287987709320227, Average Loss 0.001436\n",
      "Fuck you; It's epoch #20\n",
      " loss 0.001265 [   64 / 11478 ]\n",
      " loss 0.001267 [  613 / 11478 ]\n",
      " loss 0.001245 [ 1134 / 11478 ]\n",
      " loss 0.001637 [ 1604 / 11478 ]\n",
      " loss 0.001656 [ 2112 / 11478 ]\n",
      " loss 0.001576 [ 2590 / 11478 ]\n",
      " loss 0.001293 [ 3049 / 11478 ]\n",
      " loss 0.001462 [ 3552 / 11478 ]\n",
      " loss 0.001352 [ 4023 / 11478 ]\n",
      " loss 0.001610 [ 4566 / 11478 ]\n",
      " loss 0.001506 [ 5060 / 11478 ]\n",
      " loss 0.001506 [ 5620 / 11478 ]\n",
      " loss 0.001376 [ 6119 / 11478 ]\n",
      " loss 0.001371 [ 6612 / 11478 ]\n",
      " loss 0.001262 [ 7165 / 11478 ]\n",
      " loss 0.001356 [ 7615 / 11478 ]\n",
      " loss 0.001276 [ 8103 / 11478 ]\n",
      " loss 0.001492 [ 8660 / 11478 ]\n",
      " loss 0.001540 [ 9100 / 11478 ]\n",
      " loss 0.001289 [ 9510 / 11478 ]\n",
      " loss 0.001312 [ 9992 / 11478 ]\n",
      " loss 0.002155 [10434 / 11478 ]\n",
      " loss 0.001549 [10893 / 11478 ]\n",
      " loss 0.001330 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.31772723713393897, Average Loss 0.001387\n",
      "Fuck you; It's epoch #21\n",
      " loss 0.001294 [   64 / 11478 ]\n",
      " loss 0.001278 [  613 / 11478 ]\n",
      " loss 0.001015 [ 1134 / 11478 ]\n",
      " loss 0.001439 [ 1604 / 11478 ]\n",
      " loss 0.001338 [ 2112 / 11478 ]\n",
      " loss 0.001240 [ 2590 / 11478 ]\n",
      " loss 0.001213 [ 3049 / 11478 ]\n",
      " loss 0.001500 [ 3552 / 11478 ]\n",
      " loss 0.001214 [ 4023 / 11478 ]\n",
      " loss 0.001652 [ 4566 / 11478 ]\n",
      " loss 0.001335 [ 5060 / 11478 ]\n",
      " loss 0.001270 [ 5620 / 11478 ]\n",
      " loss 0.001248 [ 6119 / 11478 ]\n",
      " loss 0.001258 [ 6612 / 11478 ]\n",
      " loss 0.001305 [ 7165 / 11478 ]\n",
      " loss 0.001276 [ 7615 / 11478 ]\n",
      " loss 0.001297 [ 8103 / 11478 ]\n",
      " loss 0.001606 [ 8660 / 11478 ]\n",
      " loss 0.001479 [ 9100 / 11478 ]\n",
      " loss 0.001266 [ 9510 / 11478 ]\n",
      " loss 0.001473 [ 9992 / 11478 ]\n",
      " loss 0.001970 [10434 / 11478 ]\n",
      " loss 0.001308 [10893 / 11478 ]\n",
      " loss 0.001314 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.3011756044309038, Average Loss 0.001315\n",
      "Fuck you; It's epoch #22\n",
      " loss 0.001270 [   64 / 11478 ]\n",
      " loss 0.001003 [  613 / 11478 ]\n",
      " loss 0.001043 [ 1134 / 11478 ]\n",
      " loss 0.001193 [ 1604 / 11478 ]\n",
      " loss 0.001333 [ 2112 / 11478 ]\n",
      " loss 0.001214 [ 2590 / 11478 ]\n",
      " loss 0.001028 [ 3049 / 11478 ]\n",
      " loss 0.001178 [ 3552 / 11478 ]\n",
      " loss 0.001384 [ 4023 / 11478 ]\n",
      " loss 0.001334 [ 4566 / 11478 ]\n",
      " loss 0.001209 [ 5060 / 11478 ]\n",
      " loss 0.001215 [ 5620 / 11478 ]\n",
      " loss 0.001100 [ 6119 / 11478 ]\n",
      " loss 0.001291 [ 6612 / 11478 ]\n",
      " loss 0.001165 [ 7165 / 11478 ]\n",
      " loss 0.001139 [ 7615 / 11478 ]\n",
      " loss 0.001159 [ 8103 / 11478 ]\n",
      " loss 0.001105 [ 8660 / 11478 ]\n",
      " loss 0.001511 [ 9100 / 11478 ]\n",
      " loss 0.001115 [ 9510 / 11478 ]\n",
      " loss 0.001050 [ 9992 / 11478 ]\n",
      " loss 0.001299 [10434 / 11478 ]\n",
      " loss 0.000738 [10893 / 11478 ]\n",
      " loss 0.001081 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.2665361444197726, Average Loss 0.001164\n",
      "Fuck you; It's epoch #23\n",
      " loss 0.000992 [   64 / 11478 ]\n",
      " loss 0.000933 [  613 / 11478 ]\n",
      " loss 0.001056 [ 1134 / 11478 ]\n",
      " loss 0.001082 [ 1604 / 11478 ]\n",
      " loss 0.001212 [ 2112 / 11478 ]\n",
      " loss 0.001067 [ 2590 / 11478 ]\n",
      " loss 0.001057 [ 3049 / 11478 ]\n",
      " loss 0.001284 [ 3552 / 11478 ]\n",
      " loss 0.001066 [ 4023 / 11478 ]\n",
      " loss 0.001414 [ 4566 / 11478 ]\n",
      " loss 0.001050 [ 5060 / 11478 ]\n",
      " loss 0.001001 [ 5620 / 11478 ]\n",
      " loss 0.000979 [ 6119 / 11478 ]\n",
      " loss 0.000993 [ 6612 / 11478 ]\n",
      " loss 0.000994 [ 7165 / 11478 ]\n",
      " loss 0.000991 [ 7615 / 11478 ]\n",
      " loss 0.001054 [ 8103 / 11478 ]\n",
      " loss 0.000998 [ 8660 / 11478 ]\n",
      " loss 0.001384 [ 9100 / 11478 ]\n",
      " loss 0.001042 [ 9510 / 11478 ]\n",
      " loss 0.001158 [ 9992 / 11478 ]\n",
      " loss 0.001261 [10434 / 11478 ]\n",
      " loss 0.000821 [10893 / 11478 ]\n",
      " loss 0.001157 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.3003179552812656, Average Loss 0.001311\n",
      "Fuck you; It's epoch #24\n",
      " loss 0.001268 [   64 / 11478 ]\n",
      " loss 0.000943 [  613 / 11478 ]\n",
      " loss 0.001018 [ 1134 / 11478 ]\n",
      " loss 0.000972 [ 1604 / 11478 ]\n",
      " loss 0.001095 [ 2112 / 11478 ]\n",
      " loss 0.000942 [ 2590 / 11478 ]\n",
      " loss 0.000955 [ 3049 / 11478 ]\n",
      " loss 0.001629 [ 3552 / 11478 ]\n",
      " loss 0.001116 [ 4023 / 11478 ]\n",
      " loss 0.001300 [ 4566 / 11478 ]\n",
      " loss 0.001022 [ 5060 / 11478 ]\n",
      " loss 0.000991 [ 5620 / 11478 ]\n",
      " loss 0.000905 [ 6119 / 11478 ]\n",
      " loss 0.000939 [ 6612 / 11478 ]\n",
      " loss 0.000916 [ 7165 / 11478 ]\n",
      " loss 0.000941 [ 7615 / 11478 ]\n",
      " loss 0.000918 [ 8103 / 11478 ]\n",
      " loss 0.000888 [ 8660 / 11478 ]\n",
      " loss 0.001175 [ 9100 / 11478 ]\n",
      " loss 0.000956 [ 9510 / 11478 ]\n",
      " loss 0.000905 [ 9992 / 11478 ]\n",
      " loss 0.001040 [10434 / 11478 ]\n",
      " loss 0.000995 [10893 / 11478 ]\n",
      " loss 0.000986 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.26730230115355924, Average Loss 0.001167\n",
      "Fuck you; It's epoch #25\n",
      " loss 0.001113 [   64 / 11478 ]\n",
      " loss 0.000927 [  613 / 11478 ]\n",
      " loss 0.000965 [ 1134 / 11478 ]\n",
      " loss 0.000916 [ 1604 / 11478 ]\n",
      " loss 0.001176 [ 2112 / 11478 ]\n",
      " loss 0.000911 [ 2590 / 11478 ]\n",
      " loss 0.000913 [ 3049 / 11478 ]\n",
      " loss 0.001170 [ 3552 / 11478 ]\n",
      " loss 0.000869 [ 4023 / 11478 ]\n",
      " loss 0.001207 [ 4566 / 11478 ]\n",
      " loss 0.000961 [ 5060 / 11478 ]\n",
      " loss 0.000935 [ 5620 / 11478 ]\n",
      " loss 0.000874 [ 6119 / 11478 ]\n",
      " loss 0.000880 [ 6612 / 11478 ]\n",
      " loss 0.000808 [ 7165 / 11478 ]\n",
      " loss 0.000941 [ 7615 / 11478 ]\n",
      " loss 0.000862 [ 8103 / 11478 ]\n",
      " loss 0.000879 [ 8660 / 11478 ]\n",
      " loss 0.001106 [ 9100 / 11478 ]\n",
      " loss 0.000928 [ 9510 / 11478 ]\n",
      " loss 0.000891 [ 9992 / 11478 ]\n",
      " loss 0.001092 [10434 / 11478 ]\n",
      " loss 0.000995 [10893 / 11478 ]\n",
      " loss 0.001112 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.2843507959300448, Average Loss 0.001242\n",
      "Fuck you; It's epoch #26\n",
      " loss 0.001141 [   64 / 11478 ]\n",
      " loss 0.000892 [  613 / 11478 ]\n",
      " loss 0.000663 [ 1134 / 11478 ]\n",
      " loss 0.000880 [ 1604 / 11478 ]\n",
      " loss 0.001080 [ 2112 / 11478 ]\n",
      " loss 0.000850 [ 2590 / 11478 ]\n",
      " loss 0.000735 [ 3049 / 11478 ]\n",
      " loss 0.001204 [ 3552 / 11478 ]\n",
      " loss 0.000852 [ 4023 / 11478 ]\n",
      " loss 0.001150 [ 4566 / 11478 ]\n",
      " loss 0.000853 [ 5060 / 11478 ]\n",
      " loss 0.000880 [ 5620 / 11478 ]\n",
      " loss 0.000832 [ 6119 / 11478 ]\n",
      " loss 0.000820 [ 6612 / 11478 ]\n",
      " loss 0.000827 [ 7165 / 11478 ]\n",
      " loss 0.000880 [ 7615 / 11478 ]\n",
      " loss 0.000846 [ 8103 / 11478 ]\n",
      " loss 0.000830 [ 8660 / 11478 ]\n",
      " loss 0.000971 [ 9100 / 11478 ]\n",
      " loss 0.000893 [ 9510 / 11478 ]\n",
      " loss 0.000795 [ 9992 / 11478 ]\n",
      " loss 0.000982 [10434 / 11478 ]\n",
      " loss 0.000619 [10893 / 11478 ]\n",
      " loss 0.000876 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.2187627716636834, Average Loss 0.000955\n",
      "Fuck you; It's epoch #27\n",
      " loss 0.000867 [   64 / 11478 ]\n",
      " loss 0.000777 [  613 / 11478 ]\n",
      " loss 0.000780 [ 1134 / 11478 ]\n",
      " loss 0.000780 [ 1604 / 11478 ]\n",
      " loss 0.000946 [ 2112 / 11478 ]\n",
      " loss 0.000867 [ 2590 / 11478 ]\n",
      " loss 0.000792 [ 3049 / 11478 ]\n",
      " loss 0.000877 [ 3552 / 11478 ]\n",
      " loss 0.000834 [ 4023 / 11478 ]\n",
      " loss 0.001163 [ 4566 / 11478 ]\n",
      " loss 0.000932 [ 5060 / 11478 ]\n",
      " loss 0.000831 [ 5620 / 11478 ]\n",
      " loss 0.000764 [ 6119 / 11478 ]\n",
      " loss 0.000834 [ 6612 / 11478 ]\n",
      " loss 0.000789 [ 7165 / 11478 ]\n",
      " loss 0.000807 [ 7615 / 11478 ]\n",
      " loss 0.000799 [ 8103 / 11478 ]\n",
      " loss 0.000772 [ 8660 / 11478 ]\n",
      " loss 0.000940 [ 9100 / 11478 ]\n",
      " loss 0.000824 [ 9510 / 11478 ]\n",
      " loss 0.000767 [ 9992 / 11478 ]\n",
      " loss 0.000871 [10434 / 11478 ]\n",
      " loss 0.000651 [10893 / 11478 ]\n",
      " loss 0.000831 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.21463806051798887, Average Loss 0.000937\n",
      "Fuck you; It's epoch #28\n",
      " loss 0.000844 [   64 / 11478 ]\n",
      " loss 0.000769 [  613 / 11478 ]\n",
      " loss 0.000714 [ 1134 / 11478 ]\n",
      " loss 0.000789 [ 1604 / 11478 ]\n",
      " loss 0.001029 [ 2112 / 11478 ]\n",
      " loss 0.000778 [ 2590 / 11478 ]\n",
      " loss 0.000759 [ 3049 / 11478 ]\n",
      " loss 0.000812 [ 3552 / 11478 ]\n",
      " loss 0.000768 [ 4023 / 11478 ]\n",
      " loss 0.001087 [ 4566 / 11478 ]\n",
      " loss 0.000816 [ 5060 / 11478 ]\n",
      " loss 0.000777 [ 5620 / 11478 ]\n",
      " loss 0.000718 [ 6119 / 11478 ]\n",
      " loss 0.000752 [ 6612 / 11478 ]\n",
      " loss 0.000745 [ 7165 / 11478 ]\n",
      " loss 0.000767 [ 7615 / 11478 ]\n",
      " loss 0.000742 [ 8103 / 11478 ]\n",
      " loss 0.000721 [ 8660 / 11478 ]\n",
      " loss 0.000993 [ 9100 / 11478 ]\n",
      " loss 0.000769 [ 9510 / 11478 ]\n",
      " loss 0.000807 [ 9992 / 11478 ]\n",
      " loss 0.000956 [10434 / 11478 ]\n",
      " loss 0.000745 [10893 / 11478 ]\n",
      " loss 0.000786 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.2128832791937364, Average Loss 0.000930\n",
      "Fuck you; It's epoch #29\n",
      " loss 0.000814 [   64 / 11478 ]\n",
      " loss 0.000807 [  613 / 11478 ]\n",
      " loss 0.000535 [ 1134 / 11478 ]\n",
      " loss 0.000754 [ 1604 / 11478 ]\n",
      " loss 0.000941 [ 2112 / 11478 ]\n",
      " loss 0.000739 [ 2590 / 11478 ]\n",
      " loss 0.000832 [ 3049 / 11478 ]\n",
      " loss 0.000907 [ 3552 / 11478 ]\n",
      " loss 0.000746 [ 4023 / 11478 ]\n",
      " loss 0.000957 [ 4566 / 11478 ]\n",
      " loss 0.000814 [ 5060 / 11478 ]\n",
      " loss 0.000753 [ 5620 / 11478 ]\n",
      " loss 0.000724 [ 6119 / 11478 ]\n",
      " loss 0.000727 [ 6612 / 11478 ]\n",
      " loss 0.000765 [ 7165 / 11478 ]\n",
      " loss 0.000763 [ 7615 / 11478 ]\n",
      " loss 0.000718 [ 8103 / 11478 ]\n",
      " loss 0.000707 [ 8660 / 11478 ]\n",
      " loss 0.000860 [ 9100 / 11478 ]\n",
      " loss 0.000739 [ 9510 / 11478 ]\n",
      " loss 0.000723 [ 9992 / 11478 ]\n",
      " loss 0.000817 [10434 / 11478 ]\n",
      " loss 0.000669 [10893 / 11478 ]\n",
      " loss 0.000732 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.19646628277644645, Average Loss 0.000858\n",
      "Fuck you; It's epoch #30\n",
      " loss 0.000788 [   64 / 11478 ]\n",
      " loss 0.000672 [  613 / 11478 ]\n",
      " loss 0.000513 [ 1134 / 11478 ]\n",
      " loss 0.000741 [ 1604 / 11478 ]\n",
      " loss 0.000891 [ 2112 / 11478 ]\n",
      " loss 0.000718 [ 2590 / 11478 ]\n",
      " loss 0.000742 [ 3049 / 11478 ]\n",
      " loss 0.000884 [ 3552 / 11478 ]\n",
      " loss 0.000715 [ 4023 / 11478 ]\n",
      " loss 0.000915 [ 4566 / 11478 ]\n",
      " loss 0.000773 [ 5060 / 11478 ]\n",
      " loss 0.000698 [ 5620 / 11478 ]\n",
      " loss 0.000682 [ 6119 / 11478 ]\n",
      " loss 0.000682 [ 6612 / 11478 ]\n",
      " loss 0.000753 [ 7165 / 11478 ]\n",
      " loss 0.000692 [ 7615 / 11478 ]\n",
      " loss 0.000685 [ 8103 / 11478 ]\n",
      " loss 0.000691 [ 8660 / 11478 ]\n",
      " loss 0.000896 [ 9100 / 11478 ]\n",
      " loss 0.000700 [ 9510 / 11478 ]\n",
      " loss 0.000693 [ 9992 / 11478 ]\n",
      " loss 0.000843 [10434 / 11478 ]\n",
      " loss 0.000823 [10893 / 11478 ]\n",
      " loss 0.000712 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.21357454769740314, Average Loss 0.000933\n",
      "Fuck you; It's epoch #31\n",
      " loss 0.000835 [   64 / 11478 ]\n",
      " loss 0.000669 [  613 / 11478 ]\n",
      " loss 0.000863 [ 1134 / 11478 ]\n",
      " loss 0.000665 [ 1604 / 11478 ]\n",
      " loss 0.000796 [ 2112 / 11478 ]\n",
      " loss 0.000667 [ 2590 / 11478 ]\n",
      " loss 0.000657 [ 3049 / 11478 ]\n",
      " loss 0.000769 [ 3552 / 11478 ]\n",
      " loss 0.000716 [ 4023 / 11478 ]\n",
      " loss 0.000926 [ 4566 / 11478 ]\n",
      " loss 0.000744 [ 5060 / 11478 ]\n",
      " loss 0.000679 [ 5620 / 11478 ]\n",
      " loss 0.000636 [ 6119 / 11478 ]\n",
      " loss 0.000675 [ 6612 / 11478 ]\n",
      " loss 0.000686 [ 7165 / 11478 ]\n",
      " loss 0.000727 [ 7615 / 11478 ]\n",
      " loss 0.000758 [ 8103 / 11478 ]\n",
      " loss 0.000653 [ 8660 / 11478 ]\n",
      " loss 0.000791 [ 9100 / 11478 ]\n",
      " loss 0.000688 [ 9510 / 11478 ]\n",
      " loss 0.000691 [ 9992 / 11478 ]\n",
      " loss 0.000869 [10434 / 11478 ]\n",
      " loss 0.000738 [10893 / 11478 ]\n",
      " loss 0.000696 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.16670545216717184, Average Loss 0.000728\n",
      "Fuck you; It's epoch #32\n",
      " loss 0.000694 [   64 / 11478 ]\n",
      " loss 0.000642 [  613 / 11478 ]\n",
      " loss 0.000672 [ 1134 / 11478 ]\n",
      " loss 0.000631 [ 1604 / 11478 ]\n",
      " loss 0.000786 [ 2112 / 11478 ]\n",
      " loss 0.000679 [ 2590 / 11478 ]\n",
      " loss 0.000678 [ 3049 / 11478 ]\n",
      " loss 0.001426 [ 3552 / 11478 ]\n",
      " loss 0.000925 [ 4023 / 11478 ]\n",
      " loss 0.001193 [ 4566 / 11478 ]\n",
      " loss 0.000737 [ 5060 / 11478 ]\n",
      " loss 0.000673 [ 5620 / 11478 ]\n",
      " loss 0.000628 [ 6119 / 11478 ]\n",
      " loss 0.000646 [ 6612 / 11478 ]\n",
      " loss 0.000658 [ 7165 / 11478 ]\n",
      " loss 0.000662 [ 7615 / 11478 ]\n",
      " loss 0.000637 [ 8103 / 11478 ]\n",
      " loss 0.000625 [ 8660 / 11478 ]\n",
      " loss 0.000821 [ 9100 / 11478 ]\n",
      " loss 0.000697 [ 9510 / 11478 ]\n",
      " loss 0.000627 [ 9992 / 11478 ]\n",
      " loss 0.000776 [10434 / 11478 ]\n",
      " loss 0.000760 [10893 / 11478 ]\n",
      " loss 0.000703 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.16133626505892235, Average Loss 0.000705\n",
      "Fuck you; It's epoch #33\n",
      " loss 0.000642 [   64 / 11478 ]\n",
      " loss 0.000630 [  613 / 11478 ]\n",
      " loss 0.000586 [ 1134 / 11478 ]\n",
      " loss 0.000649 [ 1604 / 11478 ]\n",
      " loss 0.000773 [ 2112 / 11478 ]\n",
      " loss 0.000648 [ 2590 / 11478 ]\n",
      " loss 0.000673 [ 3049 / 11478 ]\n",
      " loss 0.001087 [ 3552 / 11478 ]\n",
      " loss 0.000692 [ 4023 / 11478 ]\n",
      " loss 0.000773 [ 4566 / 11478 ]\n",
      " loss 0.000791 [ 5060 / 11478 ]\n",
      " loss 0.000700 [ 5620 / 11478 ]\n",
      " loss 0.000593 [ 6119 / 11478 ]\n",
      " loss 0.000643 [ 6612 / 11478 ]\n",
      " loss 0.000642 [ 7165 / 11478 ]\n",
      " loss 0.000647 [ 7615 / 11478 ]\n",
      " loss 0.000632 [ 8103 / 11478 ]\n",
      " loss 0.000627 [ 8660 / 11478 ]\n",
      " loss 0.000733 [ 9100 / 11478 ]\n",
      " loss 0.000645 [ 9510 / 11478 ]\n",
      " loss 0.000695 [ 9992 / 11478 ]\n",
      " loss 0.000962 [10434 / 11478 ]\n",
      " loss 0.000788 [10893 / 11478 ]\n",
      " loss 0.000690 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.1682809016605694, Average Loss 0.000735\n",
      "Fuck you; It's epoch #34\n",
      " loss 0.000716 [   64 / 11478 ]\n",
      " loss 0.000637 [  613 / 11478 ]\n",
      " loss 0.000745 [ 1134 / 11478 ]\n",
      " loss 0.000613 [ 1604 / 11478 ]\n",
      " loss 0.000736 [ 2112 / 11478 ]\n",
      " loss 0.000637 [ 2590 / 11478 ]\n",
      " loss 0.000611 [ 3049 / 11478 ]\n",
      " loss 0.000709 [ 3552 / 11478 ]\n",
      " loss 0.000621 [ 4023 / 11478 ]\n",
      " loss 0.000776 [ 4566 / 11478 ]\n",
      " loss 0.000671 [ 5060 / 11478 ]\n",
      " loss 0.000620 [ 5620 / 11478 ]\n",
      " loss 0.000590 [ 6119 / 11478 ]\n",
      " loss 0.000622 [ 6612 / 11478 ]\n",
      " loss 0.000626 [ 7165 / 11478 ]\n",
      " loss 0.000630 [ 7615 / 11478 ]\n",
      " loss 0.000603 [ 8103 / 11478 ]\n",
      " loss 0.000599 [ 8660 / 11478 ]\n",
      " loss 0.000729 [ 9100 / 11478 ]\n",
      " loss 0.000659 [ 9510 / 11478 ]\n",
      " loss 0.000592 [ 9992 / 11478 ]\n",
      " loss 0.000722 [10434 / 11478 ]\n",
      " loss 0.000509 [10893 / 11478 ]\n",
      " loss 0.000628 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.15940981351164465, Average Loss 0.000696\n",
      "Fuck you; It's epoch #35\n",
      " loss 0.000590 [   64 / 11478 ]\n",
      " loss 0.000592 [  613 / 11478 ]\n",
      " loss 0.000499 [ 1134 / 11478 ]\n",
      " loss 0.000563 [ 1604 / 11478 ]\n",
      " loss 0.000662 [ 2112 / 11478 ]\n",
      " loss 0.000637 [ 2590 / 11478 ]\n",
      " loss 0.000594 [ 3049 / 11478 ]\n",
      " loss 0.000917 [ 3552 / 11478 ]\n",
      " loss 0.000697 [ 4023 / 11478 ]\n",
      " loss 0.000721 [ 4566 / 11478 ]\n",
      " loss 0.000669 [ 5060 / 11478 ]\n",
      " loss 0.000588 [ 5620 / 11478 ]\n",
      " loss 0.000568 [ 6119 / 11478 ]\n",
      " loss 0.000592 [ 6612 / 11478 ]\n",
      " loss 0.000609 [ 7165 / 11478 ]\n",
      " loss 0.000617 [ 7615 / 11478 ]\n",
      " loss 0.000578 [ 8103 / 11478 ]\n",
      " loss 0.000567 [ 8660 / 11478 ]\n",
      " loss 0.000722 [ 9100 / 11478 ]\n",
      " loss 0.000612 [ 9510 / 11478 ]\n",
      " loss 0.000579 [ 9992 / 11478 ]\n",
      " loss 0.000651 [10434 / 11478 ]\n",
      " loss 0.000524 [10893 / 11478 ]\n",
      " loss 0.000585 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.15173268346449903, Average Loss 0.000663\n",
      "Fuck you; It's epoch #36\n",
      " loss 0.000589 [   64 / 11478 ]\n",
      " loss 0.000577 [  613 / 11478 ]\n",
      " loss 0.000541 [ 1134 / 11478 ]\n",
      " loss 0.000590 [ 1604 / 11478 ]\n",
      " loss 0.000738 [ 2112 / 11478 ]\n",
      " loss 0.000566 [ 2590 / 11478 ]\n",
      " loss 0.000591 [ 3049 / 11478 ]\n",
      " loss 0.000799 [ 3552 / 11478 ]\n",
      " loss 0.000587 [ 4023 / 11478 ]\n",
      " loss 0.000742 [ 4566 / 11478 ]\n",
      " loss 0.000608 [ 5060 / 11478 ]\n",
      " loss 0.000585 [ 5620 / 11478 ]\n",
      " loss 0.000566 [ 6119 / 11478 ]\n",
      " loss 0.000575 [ 6612 / 11478 ]\n",
      " loss 0.000588 [ 7165 / 11478 ]\n",
      " loss 0.000596 [ 7615 / 11478 ]\n",
      " loss 0.000581 [ 8103 / 11478 ]\n",
      " loss 0.000574 [ 8660 / 11478 ]\n",
      " loss 0.000669 [ 9100 / 11478 ]\n",
      " loss 0.000569 [ 9510 / 11478 ]\n",
      " loss 0.000530 [ 9992 / 11478 ]\n",
      " loss 0.000664 [10434 / 11478 ]\n",
      " loss 0.000619 [10893 / 11478 ]\n",
      " loss 0.000583 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.14625615381553342, Average Loss 0.000639\n",
      "Fuck you; It's epoch #37\n",
      " loss 0.000593 [   64 / 11478 ]\n",
      " loss 0.000616 [  613 / 11478 ]\n",
      " loss 0.000617 [ 1134 / 11478 ]\n",
      " loss 0.000557 [ 1604 / 11478 ]\n",
      " loss 0.000642 [ 2112 / 11478 ]\n",
      " loss 0.000560 [ 2590 / 11478 ]\n",
      " loss 0.000550 [ 3049 / 11478 ]\n",
      " loss 0.000662 [ 3552 / 11478 ]\n",
      " loss 0.000543 [ 4023 / 11478 ]\n",
      " loss 0.000646 [ 4566 / 11478 ]\n",
      " loss 0.000631 [ 5060 / 11478 ]\n",
      " loss 0.000564 [ 5620 / 11478 ]\n",
      " loss 0.000560 [ 6119 / 11478 ]\n",
      " loss 0.000561 [ 6612 / 11478 ]\n",
      " loss 0.000562 [ 7165 / 11478 ]\n",
      " loss 0.000575 [ 7615 / 11478 ]\n",
      " loss 0.000530 [ 8103 / 11478 ]\n",
      " loss 0.000555 [ 8660 / 11478 ]\n",
      " loss 0.000638 [ 9100 / 11478 ]\n",
      " loss 0.000571 [ 9510 / 11478 ]\n",
      " loss 0.000579 [ 9992 / 11478 ]\n",
      " loss 0.000630 [10434 / 11478 ]\n",
      " loss 0.000633 [10893 / 11478 ]\n",
      " loss 0.000559 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.13938878510521407, Average Loss 0.000609\n",
      "Fuck you; It's epoch #38\n",
      " loss 0.000568 [   64 / 11478 ]\n",
      " loss 0.000550 [  613 / 11478 ]\n",
      " loss 0.000552 [ 1134 / 11478 ]\n",
      " loss 0.000547 [ 1604 / 11478 ]\n",
      " loss 0.000681 [ 2112 / 11478 ]\n",
      " loss 0.000554 [ 2590 / 11478 ]\n",
      " loss 0.000589 [ 3049 / 11478 ]\n",
      " loss 0.000643 [ 3552 / 11478 ]\n",
      " loss 0.000572 [ 4023 / 11478 ]\n",
      " loss 0.000676 [ 4566 / 11478 ]\n",
      " loss 0.000619 [ 5060 / 11478 ]\n",
      " loss 0.000562 [ 5620 / 11478 ]\n",
      " loss 0.000522 [ 6119 / 11478 ]\n",
      " loss 0.000546 [ 6612 / 11478 ]\n",
      " loss 0.000539 [ 7165 / 11478 ]\n",
      " loss 0.000557 [ 7615 / 11478 ]\n",
      " loss 0.000537 [ 8103 / 11478 ]\n",
      " loss 0.000529 [ 8660 / 11478 ]\n",
      " loss 0.000644 [ 9100 / 11478 ]\n",
      " loss 0.000543 [ 9510 / 11478 ]\n",
      " loss 0.000539 [ 9992 / 11478 ]\n",
      " loss 0.000578 [10434 / 11478 ]\n",
      " loss 0.000525 [10893 / 11478 ]\n",
      " loss 0.000533 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.1348081515385027, Average Loss 0.000589\n",
      "Fuck you; It's epoch #39\n",
      " loss 0.000541 [   64 / 11478 ]\n",
      " loss 0.000557 [  613 / 11478 ]\n",
      " loss 0.000427 [ 1134 / 11478 ]\n",
      " loss 0.000548 [ 1604 / 11478 ]\n",
      " loss 0.000617 [ 2112 / 11478 ]\n",
      " loss 0.000533 [ 2590 / 11478 ]\n",
      " loss 0.000563 [ 3049 / 11478 ]\n",
      " loss 0.000897 [ 3552 / 11478 ]\n",
      " loss 0.000573 [ 4023 / 11478 ]\n",
      " loss 0.000607 [ 4566 / 11478 ]\n",
      " loss 0.000604 [ 5060 / 11478 ]\n",
      " loss 0.000539 [ 5620 / 11478 ]\n",
      " loss 0.000521 [ 6119 / 11478 ]\n",
      " loss 0.000517 [ 6612 / 11478 ]\n",
      " loss 0.000555 [ 7165 / 11478 ]\n",
      " loss 0.000545 [ 7615 / 11478 ]\n",
      " loss 0.000535 [ 8103 / 11478 ]\n",
      " loss 0.000536 [ 8660 / 11478 ]\n",
      " loss 0.000621 [ 9100 / 11478 ]\n",
      " loss 0.000541 [ 9510 / 11478 ]\n",
      " loss 0.000511 [ 9992 / 11478 ]\n",
      " loss 0.000566 [10434 / 11478 ]\n",
      " loss 0.000467 [10893 / 11478 ]\n",
      " loss 0.000525 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.13990764455497995, Average Loss 0.000611\n",
      "Fuck you; It's epoch #40\n",
      " loss 0.000549 [   64 / 11478 ]\n",
      " loss 0.000521 [  613 / 11478 ]\n",
      " loss 0.000411 [ 1134 / 11478 ]\n",
      " loss 0.000528 [ 1604 / 11478 ]\n",
      " loss 0.000622 [ 2112 / 11478 ]\n",
      " loss 0.000539 [ 2590 / 11478 ]\n",
      " loss 0.000622 [ 3049 / 11478 ]\n",
      " loss 0.000750 [ 3552 / 11478 ]\n",
      " loss 0.000614 [ 4023 / 11478 ]\n",
      " loss 0.000699 [ 4566 / 11478 ]\n",
      " loss 0.000597 [ 5060 / 11478 ]\n",
      " loss 0.000533 [ 5620 / 11478 ]\n",
      " loss 0.000536 [ 6119 / 11478 ]\n",
      " loss 0.000515 [ 6612 / 11478 ]\n",
      " loss 0.000570 [ 7165 / 11478 ]\n",
      " loss 0.000535 [ 7615 / 11478 ]\n",
      " loss 0.000548 [ 8103 / 11478 ]\n",
      " loss 0.000502 [ 8660 / 11478 ]\n",
      " loss 0.000604 [ 9100 / 11478 ]\n",
      " loss 0.000523 [ 9510 / 11478 ]\n",
      " loss 0.000491 [ 9992 / 11478 ]\n",
      " loss 0.000593 [10434 / 11478 ]\n",
      " loss 0.000438 [10893 / 11478 ]\n",
      " loss 0.000525 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.13165420205944361, Average Loss 0.000575\n",
      "Fuck you; It's epoch #41\n",
      " loss 0.000519 [   64 / 11478 ]\n",
      " loss 0.000513 [  613 / 11478 ]\n",
      " loss 0.000541 [ 1134 / 11478 ]\n",
      " loss 0.000496 [ 1604 / 11478 ]\n",
      " loss 0.000599 [ 2112 / 11478 ]\n",
      " loss 0.000506 [ 2590 / 11478 ]\n",
      " loss 0.000539 [ 3049 / 11478 ]\n",
      " loss 0.000604 [ 3552 / 11478 ]\n",
      " loss 0.000516 [ 4023 / 11478 ]\n",
      " loss 0.000622 [ 4566 / 11478 ]\n",
      " loss 0.000578 [ 5060 / 11478 ]\n",
      " loss 0.000528 [ 5620 / 11478 ]\n",
      " loss 0.000527 [ 6119 / 11478 ]\n",
      " loss 0.000534 [ 6612 / 11478 ]\n",
      " loss 0.000535 [ 7165 / 11478 ]\n",
      " loss 0.000528 [ 7615 / 11478 ]\n",
      " loss 0.000514 [ 8103 / 11478 ]\n",
      " loss 0.000494 [ 8660 / 11478 ]\n",
      " loss 0.000576 [ 9100 / 11478 ]\n",
      " loss 0.000504 [ 9510 / 11478 ]\n",
      " loss 0.000470 [ 9992 / 11478 ]\n",
      " loss 0.000597 [10434 / 11478 ]\n",
      " loss 0.000447 [10893 / 11478 ]\n",
      " loss 0.000505 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.1495937141943614, Average Loss 0.000653\n",
      "Fuck you; It's epoch #42\n",
      " loss 0.000516 [   64 / 11478 ]\n",
      " loss 0.000520 [  613 / 11478 ]\n",
      " loss 0.000433 [ 1134 / 11478 ]\n",
      " loss 0.000494 [ 1604 / 11478 ]\n",
      " loss 0.000546 [ 2112 / 11478 ]\n",
      " loss 0.000504 [ 2590 / 11478 ]\n",
      " loss 0.000586 [ 3049 / 11478 ]\n",
      " loss 0.000526 [ 3552 / 11478 ]\n",
      " loss 0.000505 [ 4023 / 11478 ]\n",
      " loss 0.000572 [ 4566 / 11478 ]\n",
      " loss 0.000529 [ 5060 / 11478 ]\n",
      " loss 0.000513 [ 5620 / 11478 ]\n",
      " loss 0.000473 [ 6119 / 11478 ]\n",
      " loss 0.000490 [ 6612 / 11478 ]\n",
      " loss 0.000519 [ 7165 / 11478 ]\n",
      " loss 0.000520 [ 7615 / 11478 ]\n",
      " loss 0.000519 [ 8103 / 11478 ]\n",
      " loss 0.000493 [ 8660 / 11478 ]\n",
      " loss 0.000574 [ 9100 / 11478 ]\n",
      " loss 0.000495 [ 9510 / 11478 ]\n",
      " loss 0.000476 [ 9992 / 11478 ]\n",
      " loss 0.000550 [10434 / 11478 ]\n",
      " loss 0.000460 [10893 / 11478 ]\n",
      " loss 0.000472 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.12897857065936114, Average Loss 0.000563\n",
      "Fuck you; It's epoch #43\n",
      " loss 0.000518 [   64 / 11478 ]\n",
      " loss 0.000505 [  613 / 11478 ]\n",
      " loss 0.000362 [ 1134 / 11478 ]\n",
      " loss 0.000485 [ 1604 / 11478 ]\n",
      " loss 0.000584 [ 2112 / 11478 ]\n",
      " loss 0.000492 [ 2590 / 11478 ]\n",
      " loss 0.000502 [ 3049 / 11478 ]\n",
      " loss 0.000512 [ 3552 / 11478 ]\n",
      " loss 0.000491 [ 4023 / 11478 ]\n",
      " loss 0.000552 [ 4566 / 11478 ]\n",
      " loss 0.000541 [ 5060 / 11478 ]\n",
      " loss 0.000501 [ 5620 / 11478 ]\n",
      " loss 0.000498 [ 6119 / 11478 ]\n",
      " loss 0.000485 [ 6612 / 11478 ]\n",
      " loss 0.000523 [ 7165 / 11478 ]\n",
      " loss 0.000495 [ 7615 / 11478 ]\n",
      " loss 0.000489 [ 8103 / 11478 ]\n",
      " loss 0.000461 [ 8660 / 11478 ]\n",
      " loss 0.000582 [ 9100 / 11478 ]\n",
      " loss 0.000489 [ 9510 / 11478 ]\n",
      " loss 0.000455 [ 9992 / 11478 ]\n",
      " loss 0.000565 [10434 / 11478 ]\n",
      " loss 0.000382 [10893 / 11478 ]\n",
      " loss 0.000491 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.1262663434245921, Average Loss 0.000551\n",
      "Fuck you; It's epoch #44\n",
      " loss 0.000495 [   64 / 11478 ]\n",
      " loss 0.000490 [  613 / 11478 ]\n",
      " loss 0.000427 [ 1134 / 11478 ]\n",
      " loss 0.000471 [ 1604 / 11478 ]\n",
      " loss 0.000545 [ 2112 / 11478 ]\n",
      " loss 0.000492 [ 2590 / 11478 ]\n",
      " loss 0.000540 [ 3049 / 11478 ]\n",
      " loss 0.000489 [ 3552 / 11478 ]\n",
      " loss 0.000533 [ 4023 / 11478 ]\n",
      " loss 0.000545 [ 4566 / 11478 ]\n",
      " loss 0.000515 [ 5060 / 11478 ]\n",
      " loss 0.000489 [ 5620 / 11478 ]\n",
      " loss 0.000487 [ 6119 / 11478 ]\n",
      " loss 0.000478 [ 6612 / 11478 ]\n",
      " loss 0.000479 [ 7165 / 11478 ]\n",
      " loss 0.000488 [ 7615 / 11478 ]\n",
      " loss 0.000514 [ 8103 / 11478 ]\n",
      " loss 0.000479 [ 8660 / 11478 ]\n",
      " loss 0.000542 [ 9100 / 11478 ]\n",
      " loss 0.000490 [ 9510 / 11478 ]\n",
      " loss 0.000448 [ 9992 / 11478 ]\n",
      " loss 0.000530 [10434 / 11478 ]\n",
      " loss 0.000430 [10893 / 11478 ]\n",
      " loss 0.000474 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11986604379768497, Average Loss 0.000523\n",
      "Fuck you; It's epoch #45\n",
      " loss 0.000493 [   64 / 11478 ]\n",
      " loss 0.000497 [  613 / 11478 ]\n",
      " loss 0.000360 [ 1134 / 11478 ]\n",
      " loss 0.000454 [ 1604 / 11478 ]\n",
      " loss 0.000553 [ 2112 / 11478 ]\n",
      " loss 0.000468 [ 2590 / 11478 ]\n",
      " loss 0.000525 [ 3049 / 11478 ]\n",
      " loss 0.000499 [ 3552 / 11478 ]\n",
      " loss 0.000481 [ 4023 / 11478 ]\n",
      " loss 0.000538 [ 4566 / 11478 ]\n",
      " loss 0.000486 [ 5060 / 11478 ]\n",
      " loss 0.000482 [ 5620 / 11478 ]\n",
      " loss 0.000458 [ 6119 / 11478 ]\n",
      " loss 0.000484 [ 6612 / 11478 ]\n",
      " loss 0.000524 [ 7165 / 11478 ]\n",
      " loss 0.000471 [ 7615 / 11478 ]\n",
      " loss 0.000483 [ 8103 / 11478 ]\n",
      " loss 0.000449 [ 8660 / 11478 ]\n",
      " loss 0.000518 [ 9100 / 11478 ]\n",
      " loss 0.000464 [ 9510 / 11478 ]\n",
      " loss 0.000440 [ 9992 / 11478 ]\n",
      " loss 0.000504 [10434 / 11478 ]\n",
      " loss 0.000444 [10893 / 11478 ]\n",
      " loss 0.000475 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.1357767779503052, Average Loss 0.000593\n",
      "Fuck you; It's epoch #46\n",
      " loss 0.000508 [   64 / 11478 ]\n",
      " loss 0.000532 [  613 / 11478 ]\n",
      " loss 0.000462 [ 1134 / 11478 ]\n",
      " loss 0.000468 [ 1604 / 11478 ]\n",
      " loss 0.000535 [ 2112 / 11478 ]\n",
      " loss 0.000481 [ 2590 / 11478 ]\n",
      " loss 0.000487 [ 3049 / 11478 ]\n",
      " loss 0.000458 [ 3552 / 11478 ]\n",
      " loss 0.000472 [ 4023 / 11478 ]\n",
      " loss 0.000514 [ 4566 / 11478 ]\n",
      " loss 0.000518 [ 5060 / 11478 ]\n",
      " loss 0.000469 [ 5620 / 11478 ]\n",
      " loss 0.000457 [ 6119 / 11478 ]\n",
      " loss 0.000452 [ 6612 / 11478 ]\n",
      " loss 0.000492 [ 7165 / 11478 ]\n",
      " loss 0.000470 [ 7615 / 11478 ]\n",
      " loss 0.000467 [ 8103 / 11478 ]\n",
      " loss 0.000450 [ 8660 / 11478 ]\n",
      " loss 0.000534 [ 9100 / 11478 ]\n",
      " loss 0.000466 [ 9510 / 11478 ]\n",
      " loss 0.000424 [ 9992 / 11478 ]\n",
      " loss 0.000518 [10434 / 11478 ]\n",
      " loss 0.000407 [10893 / 11478 ]\n",
      " loss 0.000464 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11706052570111943, Average Loss 0.000511\n",
      "Fuck you; It's epoch #47\n",
      " loss 0.000475 [   64 / 11478 ]\n",
      " loss 0.000499 [  613 / 11478 ]\n",
      " loss 0.000545 [ 1134 / 11478 ]\n",
      " loss 0.000442 [ 1604 / 11478 ]\n",
      " loss 0.000513 [ 2112 / 11478 ]\n",
      " loss 0.000450 [ 2590 / 11478 ]\n",
      " loss 0.000466 [ 3049 / 11478 ]\n",
      " loss 0.000537 [ 3552 / 11478 ]\n",
      " loss 0.000497 [ 4023 / 11478 ]\n",
      " loss 0.000509 [ 4566 / 11478 ]\n",
      " loss 0.000503 [ 5060 / 11478 ]\n",
      " loss 0.000464 [ 5620 / 11478 ]\n",
      " loss 0.000460 [ 6119 / 11478 ]\n",
      " loss 0.000452 [ 6612 / 11478 ]\n",
      " loss 0.000475 [ 7165 / 11478 ]\n",
      " loss 0.000462 [ 7615 / 11478 ]\n",
      " loss 0.000470 [ 8103 / 11478 ]\n",
      " loss 0.000447 [ 8660 / 11478 ]\n",
      " loss 0.000492 [ 9100 / 11478 ]\n",
      " loss 0.000474 [ 9510 / 11478 ]\n",
      " loss 0.000432 [ 9992 / 11478 ]\n",
      " loss 0.000490 [10434 / 11478 ]\n",
      " loss 0.000412 [10893 / 11478 ]\n",
      " loss 0.000455 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.1234127878138526, Average Loss 0.000539\n",
      "Fuck you; It's epoch #48\n",
      " loss 0.000467 [   64 / 11478 ]\n",
      " loss 0.000481 [  613 / 11478 ]\n",
      " loss 0.000412 [ 1134 / 11478 ]\n",
      " loss 0.000455 [ 1604 / 11478 ]\n",
      " loss 0.000486 [ 2112 / 11478 ]\n",
      " loss 0.000460 [ 2590 / 11478 ]\n",
      " loss 0.000454 [ 3049 / 11478 ]\n",
      " loss 0.000478 [ 3552 / 11478 ]\n",
      " loss 0.000447 [ 4023 / 11478 ]\n",
      " loss 0.000507 [ 4566 / 11478 ]\n",
      " loss 0.000484 [ 5060 / 11478 ]\n",
      " loss 0.000451 [ 5620 / 11478 ]\n",
      " loss 0.000446 [ 6119 / 11478 ]\n",
      " loss 0.000445 [ 6612 / 11478 ]\n",
      " loss 0.000461 [ 7165 / 11478 ]\n",
      " loss 0.000464 [ 7615 / 11478 ]\n",
      " loss 0.000453 [ 8103 / 11478 ]\n",
      " loss 0.000442 [ 8660 / 11478 ]\n",
      " loss 0.000486 [ 9100 / 11478 ]\n",
      " loss 0.000448 [ 9510 / 11478 ]\n",
      " loss 0.000408 [ 9992 / 11478 ]\n",
      " loss 0.000496 [10434 / 11478 ]\n",
      " loss 0.000455 [10893 / 11478 ]\n",
      " loss 0.000453 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11636112565199261, Average Loss 0.000508\n",
      "Fuck you; It's epoch #49\n",
      " loss 0.000472 [   64 / 11478 ]\n",
      " loss 0.000502 [  613 / 11478 ]\n",
      " loss 0.000363 [ 1134 / 11478 ]\n",
      " loss 0.000440 [ 1604 / 11478 ]\n",
      " loss 0.000479 [ 2112 / 11478 ]\n",
      " loss 0.000453 [ 2590 / 11478 ]\n",
      " loss 0.000439 [ 3049 / 11478 ]\n",
      " loss 0.000522 [ 3552 / 11478 ]\n",
      " loss 0.000472 [ 4023 / 11478 ]\n",
      " loss 0.000471 [ 4566 / 11478 ]\n",
      " loss 0.000497 [ 5060 / 11478 ]\n",
      " loss 0.000446 [ 5620 / 11478 ]\n",
      " loss 0.000436 [ 6119 / 11478 ]\n",
      " loss 0.000426 [ 6612 / 11478 ]\n",
      " loss 0.000468 [ 7165 / 11478 ]\n",
      " loss 0.000448 [ 7615 / 11478 ]\n",
      " loss 0.000457 [ 8103 / 11478 ]\n",
      " loss 0.000441 [ 8660 / 11478 ]\n",
      " loss 0.000497 [ 9100 / 11478 ]\n",
      " loss 0.000442 [ 9510 / 11478 ]\n",
      " loss 0.000410 [ 9992 / 11478 ]\n",
      " loss 0.000468 [10434 / 11478 ]\n",
      " loss 0.000347 [10893 / 11478 ]\n",
      " loss 0.000440 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.1157478196019962, Average Loss 0.000505\n",
      "Fuck you; It's epoch #50\n",
      " loss 0.000468 [   64 / 11478 ]\n",
      " loss 0.000443 [  613 / 11478 ]\n",
      " loss 0.000339 [ 1134 / 11478 ]\n",
      " loss 0.000430 [ 1604 / 11478 ]\n",
      " loss 0.000473 [ 2112 / 11478 ]\n",
      " loss 0.000452 [ 2590 / 11478 ]\n",
      " loss 0.000464 [ 3049 / 11478 ]\n",
      " loss 0.000438 [ 3552 / 11478 ]\n",
      " loss 0.000438 [ 4023 / 11478 ]\n",
      " loss 0.000501 [ 4566 / 11478 ]\n",
      " loss 0.000517 [ 5060 / 11478 ]\n",
      " loss 0.000440 [ 5620 / 11478 ]\n",
      " loss 0.000427 [ 6119 / 11478 ]\n",
      " loss 0.000425 [ 6612 / 11478 ]\n",
      " loss 0.000468 [ 7165 / 11478 ]\n",
      " loss 0.000452 [ 7615 / 11478 ]\n",
      " loss 0.000441 [ 8103 / 11478 ]\n",
      " loss 0.000447 [ 8660 / 11478 ]\n",
      " loss 0.000483 [ 9100 / 11478 ]\n",
      " loss 0.000442 [ 9510 / 11478 ]\n",
      " loss 0.000400 [ 9992 / 11478 ]\n",
      " loss 0.000468 [10434 / 11478 ]\n",
      " loss 0.000379 [10893 / 11478 ]\n",
      " loss 0.000438 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11579716328440326, Average Loss 0.000506\n",
      "Fuck you; It's epoch #51\n",
      " loss 0.000449 [   64 / 11478 ]\n",
      " loss 0.000441 [  613 / 11478 ]\n",
      " loss 0.000461 [ 1134 / 11478 ]\n",
      " loss 0.000430 [ 1604 / 11478 ]\n",
      " loss 0.000451 [ 2112 / 11478 ]\n",
      " loss 0.000432 [ 2590 / 11478 ]\n",
      " loss 0.000430 [ 3049 / 11478 ]\n",
      " loss 0.000463 [ 3552 / 11478 ]\n",
      " loss 0.000439 [ 4023 / 11478 ]\n",
      " loss 0.000491 [ 4566 / 11478 ]\n",
      " loss 0.000493 [ 5060 / 11478 ]\n",
      " loss 0.000436 [ 5620 / 11478 ]\n",
      " loss 0.000425 [ 6119 / 11478 ]\n",
      " loss 0.000420 [ 6612 / 11478 ]\n",
      " loss 0.000488 [ 7165 / 11478 ]\n",
      " loss 0.000437 [ 7615 / 11478 ]\n",
      " loss 0.000438 [ 8103 / 11478 ]\n",
      " loss 0.000424 [ 8660 / 11478 ]\n",
      " loss 0.000497 [ 9100 / 11478 ]\n",
      " loss 0.000445 [ 9510 / 11478 ]\n",
      " loss 0.000400 [ 9992 / 11478 ]\n",
      " loss 0.000466 [10434 / 11478 ]\n",
      " loss 0.000360 [10893 / 11478 ]\n",
      " loss 0.000423 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11916787848409069, Average Loss 0.000520\n",
      "Fuck you; It's epoch #52\n",
      " loss 0.000432 [   64 / 11478 ]\n",
      " loss 0.000422 [  613 / 11478 ]\n",
      " loss 0.000362 [ 1134 / 11478 ]\n",
      " loss 0.000426 [ 1604 / 11478 ]\n",
      " loss 0.000450 [ 2112 / 11478 ]\n",
      " loss 0.000425 [ 2590 / 11478 ]\n",
      " loss 0.000453 [ 3049 / 11478 ]\n",
      " loss 0.000454 [ 3552 / 11478 ]\n",
      " loss 0.000431 [ 4023 / 11478 ]\n",
      " loss 0.000473 [ 4566 / 11478 ]\n",
      " loss 0.000459 [ 5060 / 11478 ]\n",
      " loss 0.000431 [ 5620 / 11478 ]\n",
      " loss 0.000431 [ 6119 / 11478 ]\n",
      " loss 0.000415 [ 6612 / 11478 ]\n",
      " loss 0.000443 [ 7165 / 11478 ]\n",
      " loss 0.000420 [ 7615 / 11478 ]\n",
      " loss 0.000442 [ 8103 / 11478 ]\n",
      " loss 0.000412 [ 8660 / 11478 ]\n",
      " loss 0.000431 [ 9100 / 11478 ]\n",
      " loss 0.000418 [ 9510 / 11478 ]\n",
      " loss 0.000386 [ 9992 / 11478 ]\n",
      " loss 0.000474 [10434 / 11478 ]\n",
      " loss 0.000405 [10893 / 11478 ]\n",
      " loss 0.000416 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11591687542195069, Average Loss 0.000506\n",
      "Fuck you; It's epoch #53\n",
      " loss 0.000435 [   64 / 11478 ]\n",
      " loss 0.000461 [  613 / 11478 ]\n",
      " loss 0.000340 [ 1134 / 11478 ]\n",
      " loss 0.000413 [ 1604 / 11478 ]\n",
      " loss 0.000433 [ 2112 / 11478 ]\n",
      " loss 0.000419 [ 2590 / 11478 ]\n",
      " loss 0.000433 [ 3049 / 11478 ]\n",
      " loss 0.000483 [ 3552 / 11478 ]\n",
      " loss 0.000423 [ 4023 / 11478 ]\n",
      " loss 0.000443 [ 4566 / 11478 ]\n",
      " loss 0.000491 [ 5060 / 11478 ]\n",
      " loss 0.000428 [ 5620 / 11478 ]\n",
      " loss 0.000426 [ 6119 / 11478 ]\n",
      " loss 0.000410 [ 6612 / 11478 ]\n",
      " loss 0.000490 [ 7165 / 11478 ]\n",
      " loss 0.000414 [ 7615 / 11478 ]\n",
      " loss 0.000418 [ 8103 / 11478 ]\n",
      " loss 0.000409 [ 8660 / 11478 ]\n",
      " loss 0.000456 [ 9100 / 11478 ]\n",
      " loss 0.000426 [ 9510 / 11478 ]\n",
      " loss 0.000392 [ 9992 / 11478 ]\n",
      " loss 0.000451 [10434 / 11478 ]\n",
      " loss 0.000392 [10893 / 11478 ]\n",
      " loss 0.000401 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11191183211556163, Average Loss 0.000489\n",
      "Fuck you; It's epoch #54\n",
      " loss 0.000422 [   64 / 11478 ]\n",
      " loss 0.000453 [  613 / 11478 ]\n",
      " loss 0.000337 [ 1134 / 11478 ]\n",
      " loss 0.000409 [ 1604 / 11478 ]\n",
      " loss 0.000486 [ 2112 / 11478 ]\n",
      " loss 0.000440 [ 2590 / 11478 ]\n",
      " loss 0.000434 [ 3049 / 11478 ]\n",
      " loss 0.000416 [ 3552 / 11478 ]\n",
      " loss 0.000441 [ 4023 / 11478 ]\n",
      " loss 0.000462 [ 4566 / 11478 ]\n",
      " loss 0.000472 [ 5060 / 11478 ]\n",
      " loss 0.000429 [ 5620 / 11478 ]\n",
      " loss 0.000410 [ 6119 / 11478 ]\n",
      " loss 0.000404 [ 6612 / 11478 ]\n",
      " loss 0.000456 [ 7165 / 11478 ]\n",
      " loss 0.000417 [ 7615 / 11478 ]\n",
      " loss 0.000412 [ 8103 / 11478 ]\n",
      " loss 0.000416 [ 8660 / 11478 ]\n",
      " loss 0.000443 [ 9100 / 11478 ]\n",
      " loss 0.000407 [ 9510 / 11478 ]\n",
      " loss 0.000381 [ 9992 / 11478 ]\n",
      " loss 0.000465 [10434 / 11478 ]\n",
      " loss 0.000310 [10893 / 11478 ]\n",
      " loss 0.000412 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11709550299770498, Average Loss 0.000511\n",
      "Fuck you; It's epoch #55\n",
      " loss 0.000449 [   64 / 11478 ]\n",
      " loss 0.000434 [  613 / 11478 ]\n",
      " loss 0.000453 [ 1134 / 11478 ]\n",
      " loss 0.000397 [ 1604 / 11478 ]\n",
      " loss 0.000456 [ 2112 / 11478 ]\n",
      " loss 0.000405 [ 2590 / 11478 ]\n",
      " loss 0.000405 [ 3049 / 11478 ]\n",
      " loss 0.000403 [ 3552 / 11478 ]\n",
      " loss 0.000413 [ 4023 / 11478 ]\n",
      " loss 0.000455 [ 4566 / 11478 ]\n",
      " loss 0.000469 [ 5060 / 11478 ]\n",
      " loss 0.000410 [ 5620 / 11478 ]\n",
      " loss 0.000394 [ 6119 / 11478 ]\n",
      " loss 0.000397 [ 6612 / 11478 ]\n",
      " loss 0.000441 [ 7165 / 11478 ]\n",
      " loss 0.000418 [ 7615 / 11478 ]\n",
      " loss 0.000410 [ 8103 / 11478 ]\n",
      " loss 0.000411 [ 8660 / 11478 ]\n",
      " loss 0.000445 [ 9100 / 11478 ]\n",
      " loss 0.000401 [ 9510 / 11478 ]\n",
      " loss 0.000385 [ 9992 / 11478 ]\n",
      " loss 0.000442 [10434 / 11478 ]\n",
      " loss 0.000329 [10893 / 11478 ]\n",
      " loss 0.000398 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11332641229023734, Average Loss 0.000495\n",
      "Fuck you; It's epoch #56\n",
      " loss 0.000443 [   64 / 11478 ]\n",
      " loss 0.000443 [  613 / 11478 ]\n",
      " loss 0.000329 [ 1134 / 11478 ]\n",
      " loss 0.000397 [ 1604 / 11478 ]\n",
      " loss 0.000474 [ 2112 / 11478 ]\n",
      " loss 0.000405 [ 2590 / 11478 ]\n",
      " loss 0.000419 [ 3049 / 11478 ]\n",
      " loss 0.000462 [ 3552 / 11478 ]\n",
      " loss 0.000394 [ 4023 / 11478 ]\n",
      " loss 0.000458 [ 4566 / 11478 ]\n",
      " loss 0.000450 [ 5060 / 11478 ]\n",
      " loss 0.000401 [ 5620 / 11478 ]\n",
      " loss 0.000412 [ 6119 / 11478 ]\n",
      " loss 0.000400 [ 6612 / 11478 ]\n",
      " loss 0.000469 [ 7165 / 11478 ]\n",
      " loss 0.000405 [ 7615 / 11478 ]\n",
      " loss 0.000397 [ 8103 / 11478 ]\n",
      " loss 0.000393 [ 8660 / 11478 ]\n",
      " loss 0.000421 [ 9100 / 11478 ]\n",
      " loss 0.000410 [ 9510 / 11478 ]\n",
      " loss 0.000393 [ 9992 / 11478 ]\n",
      " loss 0.000438 [10434 / 11478 ]\n",
      " loss 0.000393 [10893 / 11478 ]\n",
      " loss 0.000399 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10966633802357963, Average Loss 0.000479\n",
      "Fuck you; It's epoch #57\n",
      " loss 0.000430 [   64 / 11478 ]\n",
      " loss 0.000428 [  613 / 11478 ]\n",
      " loss 0.000293 [ 1134 / 11478 ]\n",
      " loss 0.000393 [ 1604 / 11478 ]\n",
      " loss 0.000446 [ 2112 / 11478 ]\n",
      " loss 0.000411 [ 2590 / 11478 ]\n",
      " loss 0.000416 [ 3049 / 11478 ]\n",
      " loss 0.000437 [ 3552 / 11478 ]\n",
      " loss 0.000398 [ 4023 / 11478 ]\n",
      " loss 0.000458 [ 4566 / 11478 ]\n",
      " loss 0.000446 [ 5060 / 11478 ]\n",
      " loss 0.000402 [ 5620 / 11478 ]\n",
      " loss 0.000400 [ 6119 / 11478 ]\n",
      " loss 0.000399 [ 6612 / 11478 ]\n",
      " loss 0.000430 [ 7165 / 11478 ]\n",
      " loss 0.000407 [ 7615 / 11478 ]\n",
      " loss 0.000416 [ 8103 / 11478 ]\n",
      " loss 0.000386 [ 8660 / 11478 ]\n",
      " loss 0.000430 [ 9100 / 11478 ]\n",
      " loss 0.000414 [ 9510 / 11478 ]\n",
      " loss 0.000389 [ 9992 / 11478 ]\n",
      " loss 0.000421 [10434 / 11478 ]\n",
      " loss 0.000316 [10893 / 11478 ]\n",
      " loss 0.000398 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10993012718568318, Average Loss 0.000480\n",
      "Fuck you; It's epoch #58\n",
      " loss 0.000425 [   64 / 11478 ]\n",
      " loss 0.000404 [  613 / 11478 ]\n",
      " loss 0.000301 [ 1134 / 11478 ]\n",
      " loss 0.000397 [ 1604 / 11478 ]\n",
      " loss 0.000426 [ 2112 / 11478 ]\n",
      " loss 0.000411 [ 2590 / 11478 ]\n",
      " loss 0.000397 [ 3049 / 11478 ]\n",
      " loss 0.000397 [ 3552 / 11478 ]\n",
      " loss 0.000423 [ 4023 / 11478 ]\n",
      " loss 0.000416 [ 4566 / 11478 ]\n",
      " loss 0.000423 [ 5060 / 11478 ]\n",
      " loss 0.000389 [ 5620 / 11478 ]\n",
      " loss 0.000395 [ 6119 / 11478 ]\n",
      " loss 0.000389 [ 6612 / 11478 ]\n",
      " loss 0.000467 [ 7165 / 11478 ]\n",
      " loss 0.000397 [ 7615 / 11478 ]\n",
      " loss 0.000400 [ 8103 / 11478 ]\n",
      " loss 0.000401 [ 8660 / 11478 ]\n",
      " loss 0.000445 [ 9100 / 11478 ]\n",
      " loss 0.000410 [ 9510 / 11478 ]\n",
      " loss 0.000358 [ 9992 / 11478 ]\n",
      " loss 0.000427 [10434 / 11478 ]\n",
      " loss 0.000290 [10893 / 11478 ]\n",
      " loss 0.000395 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.1116305401896705, Average Loss 0.000487\n",
      "Fuck you; It's epoch #59\n",
      " loss 0.000405 [   64 / 11478 ]\n",
      " loss 0.000397 [  613 / 11478 ]\n",
      " loss 0.000372 [ 1134 / 11478 ]\n",
      " loss 0.000388 [ 1604 / 11478 ]\n",
      " loss 0.000452 [ 2112 / 11478 ]\n",
      " loss 0.000379 [ 2590 / 11478 ]\n",
      " loss 0.000400 [ 3049 / 11478 ]\n",
      " loss 0.000408 [ 3552 / 11478 ]\n",
      " loss 0.000410 [ 4023 / 11478 ]\n",
      " loss 0.000424 [ 4566 / 11478 ]\n",
      " loss 0.000439 [ 5060 / 11478 ]\n",
      " loss 0.000406 [ 5620 / 11478 ]\n",
      " loss 0.000385 [ 6119 / 11478 ]\n",
      " loss 0.000381 [ 6612 / 11478 ]\n",
      " loss 0.000419 [ 7165 / 11478 ]\n",
      " loss 0.000399 [ 7615 / 11478 ]\n",
      " loss 0.000403 [ 8103 / 11478 ]\n",
      " loss 0.000394 [ 8660 / 11478 ]\n",
      " loss 0.000447 [ 9100 / 11478 ]\n",
      " loss 0.000398 [ 9510 / 11478 ]\n",
      " loss 0.000367 [ 9992 / 11478 ]\n",
      " loss 0.000431 [10434 / 11478 ]\n",
      " loss 0.000305 [10893 / 11478 ]\n",
      " loss 0.000395 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11527978479825944, Average Loss 0.000503\n",
      "Fuck you; It's epoch #60\n",
      " loss 0.000403 [   64 / 11478 ]\n",
      " loss 0.000421 [  613 / 11478 ]\n",
      " loss 0.000333 [ 1134 / 11478 ]\n",
      " loss 0.000381 [ 1604 / 11478 ]\n",
      " loss 0.000450 [ 2112 / 11478 ]\n",
      " loss 0.000399 [ 2590 / 11478 ]\n",
      " loss 0.000398 [ 3049 / 11478 ]\n",
      " loss 0.000429 [ 3552 / 11478 ]\n",
      " loss 0.000412 [ 4023 / 11478 ]\n",
      " loss 0.000450 [ 4566 / 11478 ]\n",
      " loss 0.000413 [ 5060 / 11478 ]\n",
      " loss 0.000381 [ 5620 / 11478 ]\n",
      " loss 0.000394 [ 6119 / 11478 ]\n",
      " loss 0.000389 [ 6612 / 11478 ]\n",
      " loss 0.000436 [ 7165 / 11478 ]\n",
      " loss 0.000403 [ 7615 / 11478 ]\n",
      " loss 0.000401 [ 8103 / 11478 ]\n",
      " loss 0.000371 [ 8660 / 11478 ]\n",
      " loss 0.000417 [ 9100 / 11478 ]\n",
      " loss 0.000393 [ 9510 / 11478 ]\n",
      " loss 0.000360 [ 9992 / 11478 ]\n",
      " loss 0.000410 [10434 / 11478 ]\n",
      " loss 0.000383 [10893 / 11478 ]\n",
      " loss 0.000386 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11143849238987222, Average Loss 0.000487\n",
      "Fuck you; It's epoch #61\n",
      " loss 0.000401 [   64 / 11478 ]\n",
      " loss 0.000415 [  613 / 11478 ]\n",
      " loss 0.000400 [ 1134 / 11478 ]\n",
      " loss 0.000387 [ 1604 / 11478 ]\n",
      " loss 0.000428 [ 2112 / 11478 ]\n",
      " loss 0.000397 [ 2590 / 11478 ]\n",
      " loss 0.000413 [ 3049 / 11478 ]\n",
      " loss 0.000399 [ 3552 / 11478 ]\n",
      " loss 0.000411 [ 4023 / 11478 ]\n",
      " loss 0.000428 [ 4566 / 11478 ]\n",
      " loss 0.000444 [ 5060 / 11478 ]\n",
      " loss 0.000384 [ 5620 / 11478 ]\n",
      " loss 0.000390 [ 6119 / 11478 ]\n",
      " loss 0.000384 [ 6612 / 11478 ]\n",
      " loss 0.000433 [ 7165 / 11478 ]\n",
      " loss 0.000386 [ 7615 / 11478 ]\n",
      " loss 0.000392 [ 8103 / 11478 ]\n",
      " loss 0.000380 [ 8660 / 11478 ]\n",
      " loss 0.000415 [ 9100 / 11478 ]\n",
      " loss 0.000382 [ 9510 / 11478 ]\n",
      " loss 0.000362 [ 9992 / 11478 ]\n",
      " loss 0.000410 [10434 / 11478 ]\n",
      " loss 0.000324 [10893 / 11478 ]\n",
      " loss 0.000386 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10919195792525717, Average Loss 0.000477\n",
      "Fuck you; It's epoch #62\n",
      " loss 0.000411 [   64 / 11478 ]\n",
      " loss 0.000405 [  613 / 11478 ]\n",
      " loss 0.000319 [ 1134 / 11478 ]\n",
      " loss 0.000376 [ 1604 / 11478 ]\n",
      " loss 0.000444 [ 2112 / 11478 ]\n",
      " loss 0.000384 [ 2590 / 11478 ]\n",
      " loss 0.000403 [ 3049 / 11478 ]\n",
      " loss 0.000395 [ 3552 / 11478 ]\n",
      " loss 0.000407 [ 4023 / 11478 ]\n",
      " loss 0.000413 [ 4566 / 11478 ]\n",
      " loss 0.000451 [ 5060 / 11478 ]\n",
      " loss 0.000381 [ 5620 / 11478 ]\n",
      " loss 0.000370 [ 6119 / 11478 ]\n",
      " loss 0.000382 [ 6612 / 11478 ]\n",
      " loss 0.000424 [ 7165 / 11478 ]\n",
      " loss 0.000388 [ 7615 / 11478 ]\n",
      " loss 0.000402 [ 8103 / 11478 ]\n",
      " loss 0.000378 [ 8660 / 11478 ]\n",
      " loss 0.000403 [ 9100 / 11478 ]\n",
      " loss 0.000387 [ 9510 / 11478 ]\n",
      " loss 0.000350 [ 9992 / 11478 ]\n",
      " loss 0.000411 [10434 / 11478 ]\n",
      " loss 0.000402 [10893 / 11478 ]\n",
      " loss 0.000385 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11195274231205515, Average Loss 0.000489\n",
      "Fuck you; It's epoch #63\n",
      " loss 0.000390 [   64 / 11478 ]\n",
      " loss 0.000423 [  613 / 11478 ]\n",
      " loss 0.000315 [ 1134 / 11478 ]\n",
      " loss 0.000386 [ 1604 / 11478 ]\n",
      " loss 0.000442 [ 2112 / 11478 ]\n",
      " loss 0.000377 [ 2590 / 11478 ]\n",
      " loss 0.000399 [ 3049 / 11478 ]\n",
      " loss 0.000402 [ 3552 / 11478 ]\n",
      " loss 0.000403 [ 4023 / 11478 ]\n",
      " loss 0.000416 [ 4566 / 11478 ]\n",
      " loss 0.000431 [ 5060 / 11478 ]\n",
      " loss 0.000377 [ 5620 / 11478 ]\n",
      " loss 0.000381 [ 6119 / 11478 ]\n",
      " loss 0.000370 [ 6612 / 11478 ]\n",
      " loss 0.000417 [ 7165 / 11478 ]\n",
      " loss 0.000382 [ 7615 / 11478 ]\n",
      " loss 0.000396 [ 8103 / 11478 ]\n",
      " loss 0.000390 [ 8660 / 11478 ]\n",
      " loss 0.000389 [ 9100 / 11478 ]\n",
      " loss 0.000382 [ 9510 / 11478 ]\n",
      " loss 0.000358 [ 9992 / 11478 ]\n",
      " loss 0.000396 [10434 / 11478 ]\n",
      " loss 0.000308 [10893 / 11478 ]\n",
      " loss 0.000370 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10733256561857919, Average Loss 0.000469\n",
      "Fuck you; It's epoch #64\n",
      " loss 0.000409 [   64 / 11478 ]\n",
      " loss 0.000390 [  613 / 11478 ]\n",
      " loss 0.000300 [ 1134 / 11478 ]\n",
      " loss 0.000389 [ 1604 / 11478 ]\n",
      " loss 0.000426 [ 2112 / 11478 ]\n",
      " loss 0.000383 [ 2590 / 11478 ]\n",
      " loss 0.000406 [ 3049 / 11478 ]\n",
      " loss 0.000382 [ 3552 / 11478 ]\n",
      " loss 0.000366 [ 4023 / 11478 ]\n",
      " loss 0.000402 [ 4566 / 11478 ]\n",
      " loss 0.000426 [ 5060 / 11478 ]\n",
      " loss 0.000382 [ 5620 / 11478 ]\n",
      " loss 0.000385 [ 6119 / 11478 ]\n",
      " loss 0.000369 [ 6612 / 11478 ]\n",
      " loss 0.000410 [ 7165 / 11478 ]\n",
      " loss 0.000388 [ 7615 / 11478 ]\n",
      " loss 0.000391 [ 8103 / 11478 ]\n",
      " loss 0.000373 [ 8660 / 11478 ]\n",
      " loss 0.000412 [ 9100 / 11478 ]\n",
      " loss 0.000367 [ 9510 / 11478 ]\n",
      " loss 0.000353 [ 9992 / 11478 ]\n",
      " loss 0.000400 [10434 / 11478 ]\n",
      " loss 0.000327 [10893 / 11478 ]\n",
      " loss 0.000376 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10706399565977341, Average Loss 0.000468\n",
      "Fuck you; It's epoch #65\n",
      " loss 0.000383 [   64 / 11478 ]\n",
      " loss 0.000413 [  613 / 11478 ]\n",
      " loss 0.000286 [ 1134 / 11478 ]\n",
      " loss 0.000386 [ 1604 / 11478 ]\n",
      " loss 0.000405 [ 2112 / 11478 ]\n",
      " loss 0.000374 [ 2590 / 11478 ]\n",
      " loss 0.000390 [ 3049 / 11478 ]\n",
      " loss 0.000410 [ 3552 / 11478 ]\n",
      " loss 0.000380 [ 4023 / 11478 ]\n",
      " loss 0.000410 [ 4566 / 11478 ]\n",
      " loss 0.000427 [ 5060 / 11478 ]\n",
      " loss 0.000385 [ 5620 / 11478 ]\n",
      " loss 0.000390 [ 6119 / 11478 ]\n",
      " loss 0.000373 [ 6612 / 11478 ]\n",
      " loss 0.000434 [ 7165 / 11478 ]\n",
      " loss 0.000380 [ 7615 / 11478 ]\n",
      " loss 0.000383 [ 8103 / 11478 ]\n",
      " loss 0.000375 [ 8660 / 11478 ]\n",
      " loss 0.000395 [ 9100 / 11478 ]\n",
      " loss 0.000369 [ 9510 / 11478 ]\n",
      " loss 0.000356 [ 9992 / 11478 ]\n",
      " loss 0.000389 [10434 / 11478 ]\n",
      " loss 0.000383 [10893 / 11478 ]\n",
      " loss 0.000382 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10873119198803392, Average Loss 0.000475\n",
      "Fuck you; It's epoch #66\n",
      " loss 0.000382 [   64 / 11478 ]\n",
      " loss 0.000401 [  613 / 11478 ]\n",
      " loss 0.000329 [ 1134 / 11478 ]\n",
      " loss 0.000378 [ 1604 / 11478 ]\n",
      " loss 0.000411 [ 2112 / 11478 ]\n",
      " loss 0.000393 [ 2590 / 11478 ]\n",
      " loss 0.000382 [ 3049 / 11478 ]\n",
      " loss 0.000388 [ 3552 / 11478 ]\n",
      " loss 0.000388 [ 4023 / 11478 ]\n",
      " loss 0.000399 [ 4566 / 11478 ]\n",
      " loss 0.000425 [ 5060 / 11478 ]\n",
      " loss 0.000380 [ 5620 / 11478 ]\n",
      " loss 0.000372 [ 6119 / 11478 ]\n",
      " loss 0.000380 [ 6612 / 11478 ]\n",
      " loss 0.000433 [ 7165 / 11478 ]\n",
      " loss 0.000376 [ 7615 / 11478 ]\n",
      " loss 0.000382 [ 8103 / 11478 ]\n",
      " loss 0.000384 [ 8660 / 11478 ]\n",
      " loss 0.000408 [ 9100 / 11478 ]\n",
      " loss 0.000367 [ 9510 / 11478 ]\n",
      " loss 0.000353 [ 9992 / 11478 ]\n",
      " loss 0.000401 [10434 / 11478 ]\n",
      " loss 0.000356 [10893 / 11478 ]\n",
      " loss 0.000370 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10770225948869414, Average Loss 0.000470\n",
      "Fuck you; It's epoch #67\n",
      " loss 0.000396 [   64 / 11478 ]\n",
      " loss 0.000402 [  613 / 11478 ]\n",
      " loss 0.000280 [ 1134 / 11478 ]\n",
      " loss 0.000363 [ 1604 / 11478 ]\n",
      " loss 0.000426 [ 2112 / 11478 ]\n",
      " loss 0.000374 [ 2590 / 11478 ]\n",
      " loss 0.000376 [ 3049 / 11478 ]\n",
      " loss 0.000402 [ 3552 / 11478 ]\n",
      " loss 0.000401 [ 4023 / 11478 ]\n",
      " loss 0.000418 [ 4566 / 11478 ]\n",
      " loss 0.000415 [ 5060 / 11478 ]\n",
      " loss 0.000390 [ 5620 / 11478 ]\n",
      " loss 0.000378 [ 6119 / 11478 ]\n",
      " loss 0.000367 [ 6612 / 11478 ]\n",
      " loss 0.000422 [ 7165 / 11478 ]\n",
      " loss 0.000375 [ 7615 / 11478 ]\n",
      " loss 0.000402 [ 8103 / 11478 ]\n",
      " loss 0.000373 [ 8660 / 11478 ]\n",
      " loss 0.000390 [ 9100 / 11478 ]\n",
      " loss 0.000371 [ 9510 / 11478 ]\n",
      " loss 0.000350 [ 9992 / 11478 ]\n",
      " loss 0.000406 [10434 / 11478 ]\n",
      " loss 0.000340 [10893 / 11478 ]\n",
      " loss 0.000369 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10639814664170318, Average Loss 0.000465\n",
      "Fuck you; It's epoch #68\n",
      " loss 0.000388 [   64 / 11478 ]\n",
      " loss 0.000410 [  613 / 11478 ]\n",
      " loss 0.000352 [ 1134 / 11478 ]\n",
      " loss 0.000370 [ 1604 / 11478 ]\n",
      " loss 0.000401 [ 2112 / 11478 ]\n",
      " loss 0.000361 [ 2590 / 11478 ]\n",
      " loss 0.000382 [ 3049 / 11478 ]\n",
      " loss 0.000397 [ 3552 / 11478 ]\n",
      " loss 0.000392 [ 4023 / 11478 ]\n",
      " loss 0.000397 [ 4566 / 11478 ]\n",
      " loss 0.000422 [ 5060 / 11478 ]\n",
      " loss 0.000381 [ 5620 / 11478 ]\n",
      " loss 0.000355 [ 6119 / 11478 ]\n",
      " loss 0.000368 [ 6612 / 11478 ]\n",
      " loss 0.000408 [ 7165 / 11478 ]\n",
      " loss 0.000387 [ 7615 / 11478 ]\n",
      " loss 0.000365 [ 8103 / 11478 ]\n",
      " loss 0.000367 [ 8660 / 11478 ]\n",
      " loss 0.000386 [ 9100 / 11478 ]\n",
      " loss 0.000368 [ 9510 / 11478 ]\n",
      " loss 0.000346 [ 9992 / 11478 ]\n",
      " loss 0.000379 [10434 / 11478 ]\n",
      " loss 0.000358 [10893 / 11478 ]\n",
      " loss 0.000369 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.1059675500795575, Average Loss 0.000463\n",
      "Fuck you; It's epoch #69\n",
      " loss 0.000380 [   64 / 11478 ]\n",
      " loss 0.000379 [  613 / 11478 ]\n",
      " loss 0.000324 [ 1134 / 11478 ]\n",
      " loss 0.000361 [ 1604 / 11478 ]\n",
      " loss 0.000415 [ 2112 / 11478 ]\n",
      " loss 0.000365 [ 2590 / 11478 ]\n",
      " loss 0.000392 [ 3049 / 11478 ]\n",
      " loss 0.000401 [ 3552 / 11478 ]\n",
      " loss 0.000382 [ 4023 / 11478 ]\n",
      " loss 0.000388 [ 4566 / 11478 ]\n",
      " loss 0.000424 [ 5060 / 11478 ]\n",
      " loss 0.000380 [ 5620 / 11478 ]\n",
      " loss 0.000373 [ 6119 / 11478 ]\n",
      " loss 0.000370 [ 6612 / 11478 ]\n",
      " loss 0.000405 [ 7165 / 11478 ]\n",
      " loss 0.000375 [ 7615 / 11478 ]\n",
      " loss 0.000396 [ 8103 / 11478 ]\n",
      " loss 0.000369 [ 8660 / 11478 ]\n",
      " loss 0.000401 [ 9100 / 11478 ]\n",
      " loss 0.000375 [ 9510 / 11478 ]\n",
      " loss 0.000337 [ 9992 / 11478 ]\n",
      " loss 0.000410 [10434 / 11478 ]\n",
      " loss 0.000312 [10893 / 11478 ]\n",
      " loss 0.000373 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10572768525879933, Average Loss 0.000462\n",
      "Fuck you; It's epoch #70\n",
      " loss 0.000371 [   64 / 11478 ]\n",
      " loss 0.000401 [  613 / 11478 ]\n",
      " loss 0.000284 [ 1134 / 11478 ]\n",
      " loss 0.000362 [ 1604 / 11478 ]\n",
      " loss 0.000417 [ 2112 / 11478 ]\n",
      " loss 0.000371 [ 2590 / 11478 ]\n",
      " loss 0.000374 [ 3049 / 11478 ]\n",
      " loss 0.000413 [ 3552 / 11478 ]\n",
      " loss 0.000394 [ 4023 / 11478 ]\n",
      " loss 0.000368 [ 4566 / 11478 ]\n",
      " loss 0.000414 [ 5060 / 11478 ]\n",
      " loss 0.000378 [ 5620 / 11478 ]\n",
      " loss 0.000387 [ 6119 / 11478 ]\n",
      " loss 0.000358 [ 6612 / 11478 ]\n",
      " loss 0.000399 [ 7165 / 11478 ]\n",
      " loss 0.000377 [ 7615 / 11478 ]\n",
      " loss 0.000381 [ 8103 / 11478 ]\n",
      " loss 0.000366 [ 8660 / 11478 ]\n",
      " loss 0.000387 [ 9100 / 11478 ]\n",
      " loss 0.000359 [ 9510 / 11478 ]\n",
      " loss 0.000343 [ 9992 / 11478 ]\n",
      " loss 0.000390 [10434 / 11478 ]\n",
      " loss 0.000244 [10893 / 11478 ]\n",
      " loss 0.000368 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11436259117505945, Average Loss 0.000499\n",
      "Fuck you; It's epoch #71\n",
      " loss 0.000370 [   64 / 11478 ]\n",
      " loss 0.000406 [  613 / 11478 ]\n",
      " loss 0.000314 [ 1134 / 11478 ]\n",
      " loss 0.000366 [ 1604 / 11478 ]\n",
      " loss 0.000401 [ 2112 / 11478 ]\n",
      " loss 0.000367 [ 2590 / 11478 ]\n",
      " loss 0.000351 [ 3049 / 11478 ]\n",
      " loss 0.000395 [ 3552 / 11478 ]\n",
      " loss 0.000366 [ 4023 / 11478 ]\n",
      " loss 0.000406 [ 4566 / 11478 ]\n",
      " loss 0.000398 [ 5060 / 11478 ]\n",
      " loss 0.000372 [ 5620 / 11478 ]\n",
      " loss 0.000369 [ 6119 / 11478 ]\n",
      " loss 0.000365 [ 6612 / 11478 ]\n",
      " loss 0.000395 [ 7165 / 11478 ]\n",
      " loss 0.000369 [ 7615 / 11478 ]\n",
      " loss 0.000371 [ 8103 / 11478 ]\n",
      " loss 0.000368 [ 8660 / 11478 ]\n",
      " loss 0.000383 [ 9100 / 11478 ]\n",
      " loss 0.000368 [ 9510 / 11478 ]\n",
      " loss 0.000334 [ 9992 / 11478 ]\n",
      " loss 0.000396 [10434 / 11478 ]\n",
      " loss 0.000328 [10893 / 11478 ]\n",
      " loss 0.000363 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10289606484128433, Average Loss 0.000449\n",
      "Fuck you; It's epoch #72\n",
      " loss 0.000373 [   64 / 11478 ]\n",
      " loss 0.000396 [  613 / 11478 ]\n",
      " loss 0.000394 [ 1134 / 11478 ]\n",
      " loss 0.000346 [ 1604 / 11478 ]\n",
      " loss 0.000401 [ 2112 / 11478 ]\n",
      " loss 0.000375 [ 2590 / 11478 ]\n",
      " loss 0.000390 [ 3049 / 11478 ]\n",
      " loss 0.000371 [ 3552 / 11478 ]\n",
      " loss 0.000374 [ 4023 / 11478 ]\n",
      " loss 0.000397 [ 4566 / 11478 ]\n",
      " loss 0.000413 [ 5060 / 11478 ]\n",
      " loss 0.000373 [ 5620 / 11478 ]\n",
      " loss 0.000368 [ 6119 / 11478 ]\n",
      " loss 0.000361 [ 6612 / 11478 ]\n",
      " loss 0.000396 [ 7165 / 11478 ]\n",
      " loss 0.000377 [ 7615 / 11478 ]\n",
      " loss 0.000375 [ 8103 / 11478 ]\n",
      " loss 0.000365 [ 8660 / 11478 ]\n",
      " loss 0.000375 [ 9100 / 11478 ]\n",
      " loss 0.000364 [ 9510 / 11478 ]\n",
      " loss 0.000343 [ 9992 / 11478 ]\n",
      " loss 0.000408 [10434 / 11478 ]\n",
      " loss 0.000341 [10893 / 11478 ]\n",
      " loss 0.000367 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10296917936802367, Average Loss 0.000450\n",
      "Fuck you; It's epoch #73\n",
      " loss 0.000378 [   64 / 11478 ]\n",
      " loss 0.000388 [  613 / 11478 ]\n",
      " loss 0.000317 [ 1134 / 11478 ]\n",
      " loss 0.000365 [ 1604 / 11478 ]\n",
      " loss 0.000440 [ 2112 / 11478 ]\n",
      " loss 0.000364 [ 2590 / 11478 ]\n",
      " loss 0.000378 [ 3049 / 11478 ]\n",
      " loss 0.000350 [ 3552 / 11478 ]\n",
      " loss 0.000369 [ 4023 / 11478 ]\n",
      " loss 0.000382 [ 4566 / 11478 ]\n",
      " loss 0.000412 [ 5060 / 11478 ]\n",
      " loss 0.000355 [ 5620 / 11478 ]\n",
      " loss 0.000377 [ 6119 / 11478 ]\n",
      " loss 0.000352 [ 6612 / 11478 ]\n",
      " loss 0.000408 [ 7165 / 11478 ]\n",
      " loss 0.000382 [ 7615 / 11478 ]\n",
      " loss 0.000376 [ 8103 / 11478 ]\n",
      " loss 0.000367 [ 8660 / 11478 ]\n",
      " loss 0.000370 [ 9100 / 11478 ]\n",
      " loss 0.000346 [ 9510 / 11478 ]\n",
      " loss 0.000332 [ 9992 / 11478 ]\n",
      " loss 0.000376 [10434 / 11478 ]\n",
      " loss 0.000349 [10893 / 11478 ]\n",
      " loss 0.000349 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10906609860121645, Average Loss 0.000476\n",
      "Fuck you; It's epoch #74\n",
      " loss 0.000383 [   64 / 11478 ]\n",
      " loss 0.000387 [  613 / 11478 ]\n",
      " loss 0.000269 [ 1134 / 11478 ]\n",
      " loss 0.000369 [ 1604 / 11478 ]\n",
      " loss 0.000403 [ 2112 / 11478 ]\n",
      " loss 0.000377 [ 2590 / 11478 ]\n",
      " loss 0.000392 [ 3049 / 11478 ]\n",
      " loss 0.000368 [ 3552 / 11478 ]\n",
      " loss 0.000371 [ 4023 / 11478 ]\n",
      " loss 0.000392 [ 4566 / 11478 ]\n",
      " loss 0.000389 [ 5060 / 11478 ]\n",
      " loss 0.000371 [ 5620 / 11478 ]\n",
      " loss 0.000368 [ 6119 / 11478 ]\n",
      " loss 0.000361 [ 6612 / 11478 ]\n",
      " loss 0.000394 [ 7165 / 11478 ]\n",
      " loss 0.000363 [ 7615 / 11478 ]\n",
      " loss 0.000385 [ 8103 / 11478 ]\n",
      " loss 0.000373 [ 8660 / 11478 ]\n",
      " loss 0.000382 [ 9100 / 11478 ]\n",
      " loss 0.000350 [ 9510 / 11478 ]\n",
      " loss 0.000338 [ 9992 / 11478 ]\n",
      " loss 0.000380 [10434 / 11478 ]\n",
      " loss 0.000377 [10893 / 11478 ]\n",
      " loss 0.000362 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10271099020275731, Average Loss 0.000449\n",
      "Fuck you; It's epoch #75\n",
      " loss 0.000366 [   64 / 11478 ]\n",
      " loss 0.000404 [  613 / 11478 ]\n",
      " loss 0.000408 [ 1134 / 11478 ]\n",
      " loss 0.000358 [ 1604 / 11478 ]\n",
      " loss 0.000417 [ 2112 / 11478 ]\n",
      " loss 0.000364 [ 2590 / 11478 ]\n",
      " loss 0.000412 [ 3049 / 11478 ]\n",
      " loss 0.000382 [ 3552 / 11478 ]\n",
      " loss 0.000385 [ 4023 / 11478 ]\n",
      " loss 0.000383 [ 4566 / 11478 ]\n",
      " loss 0.000409 [ 5060 / 11478 ]\n",
      " loss 0.000360 [ 5620 / 11478 ]\n",
      " loss 0.000369 [ 6119 / 11478 ]\n",
      " loss 0.000356 [ 6612 / 11478 ]\n",
      " loss 0.000403 [ 7165 / 11478 ]\n",
      " loss 0.000374 [ 7615 / 11478 ]\n",
      " loss 0.000377 [ 8103 / 11478 ]\n",
      " loss 0.000359 [ 8660 / 11478 ]\n",
      " loss 0.000385 [ 9100 / 11478 ]\n",
      " loss 0.000361 [ 9510 / 11478 ]\n",
      " loss 0.000335 [ 9992 / 11478 ]\n",
      " loss 0.000393 [10434 / 11478 ]\n",
      " loss 0.000266 [10893 / 11478 ]\n",
      " loss 0.000364 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10539167020447923, Average Loss 0.000460\n",
      "Fuck you; It's epoch #76\n",
      " loss 0.000369 [   64 / 11478 ]\n",
      " loss 0.000400 [  613 / 11478 ]\n",
      " loss 0.000263 [ 1134 / 11478 ]\n",
      " loss 0.000357 [ 1604 / 11478 ]\n",
      " loss 0.000398 [ 2112 / 11478 ]\n",
      " loss 0.000364 [ 2590 / 11478 ]\n",
      " loss 0.000380 [ 3049 / 11478 ]\n",
      " loss 0.000338 [ 3552 / 11478 ]\n",
      " loss 0.000361 [ 4023 / 11478 ]\n",
      " loss 0.000389 [ 4566 / 11478 ]\n",
      " loss 0.000400 [ 5060 / 11478 ]\n",
      " loss 0.000359 [ 5620 / 11478 ]\n",
      " loss 0.000365 [ 6119 / 11478 ]\n",
      " loss 0.000360 [ 6612 / 11478 ]\n",
      " loss 0.000409 [ 7165 / 11478 ]\n",
      " loss 0.000366 [ 7615 / 11478 ]\n",
      " loss 0.000360 [ 8103 / 11478 ]\n",
      " loss 0.000372 [ 8660 / 11478 ]\n",
      " loss 0.000370 [ 9100 / 11478 ]\n",
      " loss 0.000365 [ 9510 / 11478 ]\n",
      " loss 0.000352 [ 9992 / 11478 ]\n",
      " loss 0.000395 [10434 / 11478 ]\n",
      " loss 0.000267 [10893 / 11478 ]\n",
      " loss 0.000358 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10219010922470918, Average Loss 0.000446\n",
      "Fuck you; It's epoch #77\n",
      " loss 0.000378 [   64 / 11478 ]\n",
      " loss 0.000391 [  613 / 11478 ]\n",
      " loss 0.000314 [ 1134 / 11478 ]\n",
      " loss 0.000359 [ 1604 / 11478 ]\n",
      " loss 0.000388 [ 2112 / 11478 ]\n",
      " loss 0.000357 [ 2590 / 11478 ]\n",
      " loss 0.000367 [ 3049 / 11478 ]\n",
      " loss 0.000370 [ 3552 / 11478 ]\n",
      " loss 0.000344 [ 4023 / 11478 ]\n",
      " loss 0.000372 [ 4566 / 11478 ]\n",
      " loss 0.000401 [ 5060 / 11478 ]\n",
      " loss 0.000373 [ 5620 / 11478 ]\n",
      " loss 0.000360 [ 6119 / 11478 ]\n",
      " loss 0.000360 [ 6612 / 11478 ]\n",
      " loss 0.000406 [ 7165 / 11478 ]\n",
      " loss 0.000360 [ 7615 / 11478 ]\n",
      " loss 0.000387 [ 8103 / 11478 ]\n",
      " loss 0.000362 [ 8660 / 11478 ]\n",
      " loss 0.000366 [ 9100 / 11478 ]\n",
      " loss 0.000358 [ 9510 / 11478 ]\n",
      " loss 0.000331 [ 9992 / 11478 ]\n",
      " loss 0.000378 [10434 / 11478 ]\n",
      " loss 0.000305 [10893 / 11478 ]\n",
      " loss 0.000360 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11007818868384747, Average Loss 0.000481\n",
      "Fuck you; It's epoch #78\n",
      " loss 0.000371 [   64 / 11478 ]\n",
      " loss 0.000383 [  613 / 11478 ]\n",
      " loss 0.000248 [ 1134 / 11478 ]\n",
      " loss 0.000353 [ 1604 / 11478 ]\n",
      " loss 0.000406 [ 2112 / 11478 ]\n",
      " loss 0.000354 [ 2590 / 11478 ]\n",
      " loss 0.000376 [ 3049 / 11478 ]\n",
      " loss 0.000404 [ 3552 / 11478 ]\n",
      " loss 0.000368 [ 4023 / 11478 ]\n",
      " loss 0.000383 [ 4566 / 11478 ]\n",
      " loss 0.000399 [ 5060 / 11478 ]\n",
      " loss 0.000369 [ 5620 / 11478 ]\n",
      " loss 0.000360 [ 6119 / 11478 ]\n",
      " loss 0.000355 [ 6612 / 11478 ]\n",
      " loss 0.000398 [ 7165 / 11478 ]\n",
      " loss 0.000357 [ 7615 / 11478 ]\n",
      " loss 0.000358 [ 8103 / 11478 ]\n",
      " loss 0.000366 [ 8660 / 11478 ]\n",
      " loss 0.000380 [ 9100 / 11478 ]\n",
      " loss 0.000356 [ 9510 / 11478 ]\n",
      " loss 0.000330 [ 9992 / 11478 ]\n",
      " loss 0.000383 [10434 / 11478 ]\n",
      " loss 0.000299 [10893 / 11478 ]\n",
      " loss 0.000355 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10621241329612484, Average Loss 0.000464\n",
      "Fuck you; It's epoch #79\n",
      " loss 0.000365 [   64 / 11478 ]\n",
      " loss 0.000375 [  613 / 11478 ]\n",
      " loss 0.000342 [ 1134 / 11478 ]\n",
      " loss 0.000356 [ 1604 / 11478 ]\n",
      " loss 0.000402 [ 2112 / 11478 ]\n",
      " loss 0.000366 [ 2590 / 11478 ]\n",
      " loss 0.000358 [ 3049 / 11478 ]\n",
      " loss 0.000372 [ 3552 / 11478 ]\n",
      " loss 0.000382 [ 4023 / 11478 ]\n",
      " loss 0.000365 [ 4566 / 11478 ]\n",
      " loss 0.000418 [ 5060 / 11478 ]\n",
      " loss 0.000360 [ 5620 / 11478 ]\n",
      " loss 0.000371 [ 6119 / 11478 ]\n",
      " loss 0.000346 [ 6612 / 11478 ]\n",
      " loss 0.000393 [ 7165 / 11478 ]\n",
      " loss 0.000372 [ 7615 / 11478 ]\n",
      " loss 0.000370 [ 8103 / 11478 ]\n",
      " loss 0.000352 [ 8660 / 11478 ]\n",
      " loss 0.000388 [ 9100 / 11478 ]\n",
      " loss 0.000351 [ 9510 / 11478 ]\n",
      " loss 0.000334 [ 9992 / 11478 ]\n",
      " loss 0.000373 [10434 / 11478 ]\n",
      " loss 0.000334 [10893 / 11478 ]\n",
      " loss 0.000353 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10551327188405635, Average Loss 0.000461\n",
      "Fuck you; It's epoch #80\n",
      " loss 0.000379 [   64 / 11478 ]\n",
      " loss 0.000366 [  613 / 11478 ]\n",
      " loss 0.000345 [ 1134 / 11478 ]\n",
      " loss 0.000356 [ 1604 / 11478 ]\n",
      " loss 0.000412 [ 2112 / 11478 ]\n",
      " loss 0.000371 [ 2590 / 11478 ]\n",
      " loss 0.000393 [ 3049 / 11478 ]\n",
      " loss 0.000378 [ 3552 / 11478 ]\n",
      " loss 0.000381 [ 4023 / 11478 ]\n",
      " loss 0.000382 [ 4566 / 11478 ]\n",
      " loss 0.000408 [ 5060 / 11478 ]\n",
      " loss 0.000367 [ 5620 / 11478 ]\n",
      " loss 0.000370 [ 6119 / 11478 ]\n",
      " loss 0.000347 [ 6612 / 11478 ]\n",
      " loss 0.000399 [ 7165 / 11478 ]\n",
      " loss 0.000366 [ 7615 / 11478 ]\n",
      " loss 0.000359 [ 8103 / 11478 ]\n",
      " loss 0.000355 [ 8660 / 11478 ]\n",
      " loss 0.000366 [ 9100 / 11478 ]\n",
      " loss 0.000359 [ 9510 / 11478 ]\n",
      " loss 0.000334 [ 9992 / 11478 ]\n",
      " loss 0.000367 [10434 / 11478 ]\n",
      " loss 0.000293 [10893 / 11478 ]\n",
      " loss 0.000354 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.10444692613793476, Average Loss 0.000456\n",
      "Fuck you; It's epoch #81\n",
      " loss 0.000390 [   64 / 11478 ]\n",
      " loss 0.000409 [  613 / 11478 ]\n",
      " loss 0.000292 [ 1134 / 11478 ]\n",
      " loss 0.000356 [ 1604 / 11478 ]\n",
      " loss 0.000398 [ 2112 / 11478 ]\n",
      " loss 0.000368 [ 2590 / 11478 ]\n",
      " loss 0.000373 [ 3049 / 11478 ]\n",
      " loss 0.000368 [ 3552 / 11478 ]\n",
      " loss 0.000345 [ 4023 / 11478 ]\n",
      " loss 0.000377 [ 4566 / 11478 ]\n",
      " loss 0.000417 [ 5060 / 11478 ]\n",
      " loss 0.000358 [ 5620 / 11478 ]\n",
      " loss 0.000362 [ 6119 / 11478 ]\n",
      " loss 0.000349 [ 6612 / 11478 ]\n",
      " loss 0.000401 [ 7165 / 11478 ]\n",
      " loss 0.000354 [ 7615 / 11478 ]\n",
      " loss 0.000360 [ 8103 / 11478 ]\n",
      " loss 0.000363 [ 8660 / 11478 ]\n",
      " loss 0.000376 [ 9100 / 11478 ]\n",
      " loss 0.000353 [ 9510 / 11478 ]\n",
      " loss 0.000334 [ 9992 / 11478 ]\n",
      " loss 0.000375 [10434 / 11478 ]\n",
      " loss 0.000308 [10893 / 11478 ]\n",
      " loss 0.000353 [11401 / 11478 ]\n",
      "Test set: \n",
      "Cumulative loss: 0.11012414764547772, Average Loss 0.000481\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "warmup = 20\n",
    "\n",
    "# Linear increase to 1e-4 until warmup finished, then exp decay for remainder of time\n",
    "L1 = lambda epoch: (epoch+1) / (warmup + 1) if epoch < warmup else 2.1718**((warmup - epoch)/10)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, L1)\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "training_loss = torch.zeros(n_epochs)\n",
    "val_loss = torch.zeros(n_epochs)\n",
    "\n",
    "min_val_loss = 10000\n",
    "patience = 5\n",
    "val_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Fuck you; It\\'s epoch #{epoch + 1}')\n",
    "    current_training_loss = train_loop(model, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer)\n",
    "    current_val_loss = test_loop(model, dataloader=validation_dataloader, criterion=criterion)\n",
    "\n",
    "    training_loss[epoch] = current_training_loss\n",
    "    val_loss[epoch]  = current_val_loss\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if current_val_loss > min_val_loss:\n",
    "        val_counter += 1\n",
    "        if val_counter == patience: \n",
    "            break\n",
    "    else:\n",
    "        val_counter = 0\n",
    "        min_val_loss = current_val_loss\n",
    "\n",
    "\n",
    "save_after_training = True\n",
    "if save_after_training:\n",
    "    loss_dict = {\n",
    "        'training_loss': training_loss, \n",
    "        'val_loss': val_loss\n",
    "        }\n",
    "    save_string = 'bbs_v2'\n",
    "    torch.save(model.state_dict(), f'./traj_trans_1807_{save_string}.pt')\n",
    "    torch.save(loss_dict, f'./loss_history_1807_{save_string}.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_paths( model, val_loader, t_max, criterion ):\n",
    "    model.eval()\n",
    "\n",
    "    rand_sample = next(iter(val_loader))[0]\n",
    "    rand_sample = rand_sample.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        recon_sample = model(rand_sample)\n",
    "\n",
    "    recon_sample = recon_sample.squeeze()\n",
    "    rand_sample = rand_sample.squeeze()\n",
    "    \n",
    "    meas_x, meas_y = [], []\n",
    "    model_x, model_y = [], []\n",
    "\n",
    "    for i in range(len(rand_sample)):\n",
    "        meas_x.append(rand_sample[i,0].item())\n",
    "        meas_y.append(rand_sample[i,1].item())\n",
    "        model_x.append(recon_sample[i,0].item())\n",
    "        model_y.append(recon_sample[i,1].item())\n",
    "\n",
    "\n",
    "    mse_error = criterion(rand_sample, recon_sample)\n",
    "\n",
    "    plt.plot(meas_x, meas_y, 'b', label = \"True traj\")\n",
    "    plt.plot(model_x, model_y, 'k', label = \"Est path\")\n",
    "    plt.title(f\"MSE = {mse_error:.6f}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_some_paths(model, validation_dataloader, max_seq_time, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
