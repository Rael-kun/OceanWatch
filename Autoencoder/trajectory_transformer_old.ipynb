{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First implementation of a trajectory transformer, where each observation is a vector of [lat lon, sog, cog]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from typing import Iterator, List\n",
    "\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as f\n",
    "import torch.nn.functional as F\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from tqdm.notebook import tqdm as tnb\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (could move to config file if wanna be fancy)\n",
    "\n",
    "train_path = '../../temp/marinedata_train.pkl'\n",
    "val_path = '../../temp/marinedata_val.pkl'\n",
    "\n",
    "d_model = 128 # Size of the embedding space\n",
    "max_seq_time = 144 # Maximum number of observations per trajectory\n",
    "min_seq_time = 24\n",
    "input_dim = 4 # [lat, lon, sog, cog]\n",
    "output_dim = input_dim # [lat, lon, sog, cog]\n",
    "n_enc_layers = 6 \n",
    "n_dec_layers = n_enc_layers\n",
    "n_heads = 8 # number of attentionheads in the encoder and decoder\n",
    "d_ff = 512 # Dimension of the ff nn at the end of a block\n",
    "d_latent = 32 # Dimension of the latent space (Perhaps uneccesary)\n",
    "\n",
    "batch_by_size = True\n",
    "batch_size = 64\n",
    "\n",
    "params = [input_dim, d_model, n_heads, n_enc_layers, n_dec_layers, d_ff, max_seq_time, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Postional encoder using the sinusiodals from the original paper\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype = torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-torch.log(torch.tensor(10000.0))/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        self.pe = pe.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TrajectoryEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Encoder relying mainly on pytorchs modules \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model, n_heads, n_enc_layers, d_ff, max_len):\n",
    "        super(TrajectoryEncoder, self).__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        enc_layers = nn.TransformerEncoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(enc_layers, n_enc_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = self.input_proj(src) # Take our input from input_dim to embedding-d\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, n_heads, n_dec_layers, d_ff, max_len, \n",
    "                 output_dim):\n",
    "        super(TrajectoryDecoder, self).__init__()\n",
    "        self.emb_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        dec_layers = nn.TransformerDecoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(dec_layers, n_dec_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        \"\"\"\n",
    "        tgt (Tensor) = the sequence to the decoder (required).\n",
    "\n",
    "        memory (Tensor) = the sequence from the last layer of the encoder. Size bs, T, n_emb\n",
    "        \"\"\"\n",
    "        tgt = self.emb_proj(tgt)\n",
    "        target = self.pos_enc(tgt)\n",
    "        decoded = self.transformer_decoder(target, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "        output = self.output_proj(decoded) # project down to output_dim\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads, n_enc_layers, n_dec_layers, d_ff, max_len, output_dim):\n",
    "        super(TrajectoryAutoencoder, self).__init__()\n",
    "        self.encoder = TrajectoryEncoder(input_dim, d_model, n_heads, n_enc_layers, d_ff, max_len)\n",
    "        self.decoder = TrajectoryDecoder(input_dim, d_model, n_heads, n_dec_layers, d_ff, max_len, output_dim)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        memory = self.encoder(tgt)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1))\n",
    "        reconstructed = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time = min_seq_time\n",
    "if min_time>max_seq_time:\n",
    "    min_time = max_seq_time\n",
    "\n",
    "class MarineDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.data[idx]).to(device)\n",
    "\n",
    "class MarineDatasetPadding(Dataset):\n",
    "    def __init__(self, data_dictionary, min_seq_len, max_seq_len, n_features):\n",
    "\n",
    "        self.min_seq_len = min_seq_len\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.dict = data_dictionary\n",
    "        self.keys = list(data_dictionary.keys())\n",
    "\n",
    "        for key in self.keys:\n",
    "            if len(self.dict[key]['traj']) < min_seq_len:\n",
    "                del self.dict[key]\n",
    "                self.keys.remove(key)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        traj_data = self.dict[key]['traj']\n",
    "        \n",
    "        # Cut off the mmsi and timestamp and return trajectory\n",
    "        if len(traj_data) >= self.max_seq_len:\n",
    "            return torch.tensor(traj_data[:self.max_seq_len,:self.n_features])\n",
    "        else: \n",
    "            needed_padding = self.max_seq_len - len(traj_data)\n",
    "            res = F.pad(torch.tensor(traj_data[:,:self.n_features]), (0, 0, 0, needed_padding), \"constant\", 0.0)\n",
    "\n",
    "            return res.to(device)\n",
    "\n",
    "class BatchBySizeSampler(Sampler[List[int]]):\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        paths_by_length = defaultdict(list)\n",
    "        for i, path in enumerate(data):\n",
    "            paths_by_length[len(path)].append(i)\n",
    "        self.paths_by_length = list(paths_by_length.values())\n",
    "        self.num_batches = sum((len(xs) + batch_size - 1) // batch_size for xs in self.paths_by_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        for xs in self.paths_by_length:\n",
    "            np.random.shuffle(xs)\n",
    "        chunks = [xs[i : i + self.batch_size] for xs in self.paths_by_length for i in range(0, len(xs), self.batch_size)]\n",
    "        np.random.shuffle(chunks)\n",
    "        yield from chunks\n",
    "\n",
    "\n",
    "def merge_path_lists(l1, l2):\n",
    "    inner_1 = len(l1[0])\n",
    "    inner_2 = len(l2[0])\n",
    "    if inner_1 > inner_2:\n",
    "        l1, l2 = l2, l1\n",
    "    target_inner = len(l1[0])\n",
    "    for p in l2:\n",
    "        l1.append(p[:target_inner])\n",
    "    return l1\n",
    "\n",
    "def merge_lengths(all_paths):\n",
    "    paths_by_length = defaultdict(list)\n",
    "    for path in all_paths:\n",
    "        paths_by_length[len(path)].append(path)\n",
    "    paths_by_length = list(paths_by_length.values())\n",
    "    paths_by_length.sort(key=len)\n",
    "    fixed = []\n",
    "    i = 0\n",
    "    while i < len(paths_by_length):\n",
    "        curr = paths_by_length[i]\n",
    "        \"\"\"\n",
    "        maybe make each bucket strictly larger than batch size (currently equal is allowed)?\n",
    "         - this would prevent batches with exact same items between epochs\n",
    "         - problem is it would lead to small \"excess batches\", that is batches with fewer than batch_size #paths in them\n",
    "         - for example, bucket of size (batch_size + 1) leads to one batch with batch_size #paths and one with a single path\n",
    "                => might mess with gradient step\n",
    "        \"\"\"\n",
    "        while len(curr) < batch_size:\n",
    "            curr = merge_path_lists(curr, paths_by_length[i + 1])\n",
    "            i += 1\n",
    "        fixed.append(curr)\n",
    "        i += 1\n",
    "    return [x for xs in fixed for x in xs]\n",
    "\n",
    "\n",
    "if batch_by_size:\n",
    "    with open(train_path, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "        train_data = [x[\"traj\"][:max_seq_time,:input_dim] for x in train_data.values() if len(x[\"traj\"]) >= min_seq_time]\n",
    "    with open(val_path, 'rb') as f:\n",
    "        val_data = pickle.load(f)\n",
    "        val_data = [x[\"traj\"][:max_seq_time,:input_dim] for x in val_data.values() if len(x[\"traj\"]) >= min_seq_time]\n",
    "\n",
    "    train_merged = merge_lengths(train_data)\n",
    "    val_merged = merge_lengths(val_data)\n",
    "    \n",
    "    train_dataloader = DataLoader(MarineDataset(train_merged), batch_sampler=BatchBySizeSampler(train_merged, batch_size))\n",
    "    validation_dataloader = DataLoader(MarineDataset(val_merged), batch_sampler=BatchBySizeSampler(val_merged, batch_size))\n",
    "else:\n",
    "    with open(train_path, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(val_path, 'rb') as f:\n",
    "        val_data = pickle.load(f)\n",
    "    train_dataset = MarineDatasetPadding(train_data, min_seq_len=min_seq_time, max_seq_len=max_seq_time, n_features=input_dim)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataset = MarineDatasetPadding(val_data, min_seq_len=min_seq_time, max_seq_len=max_seq_time, n_features=input_dim)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lat min 53.4 max 66.2\n",
    "# lon min 9.4 max 30.5\n",
    "# Max speed 50\n",
    "# Max heading 360\n",
    "\n",
    "def train_loop(model, dataloader, criterion, optimizer):\n",
    "    train_data_size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    acc = 0\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc += len(X)\n",
    "        if batch % 2 == 0:\n",
    "            print(f' loss {loss.item():>4f} [{acc:>5d} / {train_data_size:>5d} ]', flush=True)\n",
    "\n",
    "\n",
    "def test_loop(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    loss = []\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            pred = model(X)\n",
    "            loss.append(criterion(X, pred).item())\n",
    "    print(f'Test set: \\nCumulative loss: {cum_loss}, Average Loss {np.mean(loss):>8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrajectoryAutoencoder(*params)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuck you; It's epoch #1\n",
      " loss 1.565751 [   64 / 11479 ]\n",
      " loss 0.205048 [  136 / 11479 ]\n",
      " loss 0.210800 [  264 / 11479 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuck you; It\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms epoch #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     test_loop(model, dataloader\u001b[38;5;241m=\u001b[39mvalidation_dataloader, criterion\u001b[38;5;241m=\u001b[39mcriterion)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLGF!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 15\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, X)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/edap30-cpu/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/edap30-cpu/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Fuck you; It\\'s epoch #{epoch + 1}')\n",
    "    train_loop(model, dataloader=train_dataloader, criterion=criterion, optimizer=optimizer)\n",
    "    test_loop(model, dataloader=validation_dataloader, criterion=criterion)\n",
    "\n",
    "print('LGF!')\n",
    "\n",
    "if True:\n",
    "    torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_paths(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    rand_sample = next(iter(val_loader))[0]\n",
    "    rand_sample = rand_sample.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        recon_sample = model(rand_sample)\n",
    "\n",
    "    recon_sample = recon_sample.squeeze()\n",
    "    rand_sample = rand_sample.squeeze()\n",
    "    \n",
    "    meas_x, meas_y = [], []\n",
    "    model_x, model_y = [], []\n",
    "\n",
    "    for i in range(len(rand_sample)):\n",
    "        meas_x.append(rand_sample[i,0].item())\n",
    "        meas_y.append(rand_sample[i,1].item())\n",
    "        model_x.append(recon_sample[i,0].item())\n",
    "        model_y.append(recon_sample[i,1].item())\n",
    "\n",
    "\n",
    "    mse_error = criterion(rand_sample, recon_sample)\n",
    "\n",
    "    plt.plot(meas_x, meas_y, 'b', label = \"True traj\")\n",
    "    plt.plot(model_x, model_y, 'k', label = \"Est path\")\n",
    "    plt.title(f\"MSE = {mse_error:.6f}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [64, 4] at entry 0 and [36, 4] at entry 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_some_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36mplot_some_paths\u001b[0;34m(model, val_loader, criterion)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_some_paths\u001b[39m(model, val_loader, criterion):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m     rand_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     rand_sample \u001b[38;5;241m=\u001b[39m rand_sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/edap30-cpu/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/edap30-cpu/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/edap30-cpu/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/edap30-cpu/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/edap30-cpu/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/edap30-cpu/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [64, 4] at entry 0 and [36, 4] at entry 7"
     ]
    }
   ],
   "source": [
    "plot_some_paths(model, validation_dataloader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
